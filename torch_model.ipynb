{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58491147-580d-409e-aedf-2d49997dfea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch-geometric)\n",
      "  Using cached aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: fsspec in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (6.0.0)\n",
      "Requirement already satisfied: pyparsing in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: requests in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (2.32.3)\n",
      "Collecting tqdm (from torch-geometric)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->torch-geometric)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from aiohttp->torch-geometric) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch-geometric)\n",
      "  Using cached propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch-geometric)\n",
      "  Using cached yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from jinja2->torch-geometric) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (2024.8.30)\n",
      "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "Using cached aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Using cached propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Using cached yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Installing collected packages: tqdm, propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch-geometric\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 frozenlist-1.5.0 multidict-6.1.0 propcache-0.2.1 torch-geometric-2.6.1 tqdm-4.67.1 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04f8ed93-0f7e-4ded-8b2a-73e2374eb7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import relu, sigmoid, binary_cross_entropy\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b56272de-cf3d-409c-87a2-ee7fcb82fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self,  l1, l2, l3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(l1, l2)\n",
    "        self.conv2 = GCNConv(l2, l3)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class EdgesMLP(torch.nn.Module):\n",
    "    def __init__(self, l3):\n",
    "        super(EdgesMLP, self).__init__()\n",
    "        self.linear = Linear(2*l3, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear(x)\n",
    "        return sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d873f8d-d98c-4325-9aac-3cc68cb7b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET INFO:\n",
      "count row: 1557\n",
      "first: dict_keys(['A', 'nodes_feature', 'edges_feature', 'true_edges'])\n",
      "\t A: (2, 779)\n",
      "\t nodes_feature: (385, 9)\n",
      "\t edges_feature: (779,)\n",
      "\t true_edges: (779,)\n",
      "end: dict_keys(['A', 'nodes_feature', 'edges_feature', 'true_edges'])\n",
      "\t A: (2, 2142)\n",
      "\t nodes_feature: (1039, 9)\n",
      "\t edges_feature: (2142,)\n",
      "\t true_edges: (2142,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)['dataset']\n",
    "\n",
    "\n",
    "print(\"DATASET INFO:\")\n",
    "print(\"count row:\", len(dataset))\n",
    "print(\"first:\", dataset[0].keys())\n",
    "print(f\"\\t A:\", np.shape(dataset[0][\"A\"]))\n",
    "print(f\"\\t nodes_feature:\", np.shape(dataset[0][\"nodes_feature\"]))\n",
    "print(f\"\\t edges_feature:\", np.shape(dataset[0][\"edges_feature\"]))\n",
    "print(f\"\\t true_edges:\", np.shape(dataset[0][\"true_edges\"]))\n",
    "print(\"end:\", dataset[-1].keys())\n",
    "print(f\"\\t A:\", np.shape(dataset[-1][\"A\"]))\n",
    "print(f\"\\t nodes_feature:\", np.shape(dataset[-1][\"nodes_feature\"]))\n",
    "print(f\"\\t edges_feature:\", np.shape(dataset[-1][\"edges_feature\"]))\n",
    "print(f\"\\t true_edges:\", np.shape(dataset[-1][\"true_edges\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5096a10-c1be-42c4-9a04-fc6d776ea7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_dist(a):\n",
    "    if a==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/a\n",
    "        \n",
    "i = dataset[0][\"A\"]\n",
    "v_in = [rev_dist(e) for e in dataset[0][\"edges_feature\"]]\n",
    "v_true = dataset[0][\"true_edges\"]\n",
    "x = dataset[0][\"nodes_feature\"]\n",
    "N = len(x)\n",
    "\n",
    "X = torch.Tensor(x)\n",
    "sp_A = torch.sparse_coo_tensor(i, v_in, (N, N))\n",
    "E_true = torch.Tensor([v_true]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb1c09-c6e5-4280-ac3e-b5763b89b46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d52fdd-73be-49dc-9e20-23976e4d3142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "054d8282-2dfa-47ab-a0cc-3cbe5ddcadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"count_neuron_layer_1\": 9,\n",
    "    \"count_neuron_layer_2\": 27,\n",
    "    \"count_neuron_layer_end\": 18,\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 100,\n",
    "}\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "l1 = params[\"count_neuron_layer_1\"]\n",
    "l2 = params[\"count_neuron_layer_2\"]\n",
    "l3 = params[\"count_neuron_layer_end\"]\n",
    "\n",
    "node_gnn = GNN(l1, l2, l3)\n",
    "edge_linear = EdgesMLP(l3)\n",
    "\n",
    "H_end = node_gnn(X, sp_A)\n",
    "Omega = torch.cat([H_end[i[0]], H_end[i[1]]],dim=1)\n",
    "E_pred = edge_linear(Omega)\n",
    "loss = binary_cross_entropy(E_pred, E_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a52b052a-e399-4f4d-951e-abe3e5053531",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    list(node_gnn.parameters()) + list(edge_linear.parameters()),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5e60566c-0723-4b95-810b-594ac69c22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_batchs(dataset, batch_size):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i:i+batch_size]\n",
    "\n",
    "def split_train_val(dataset, val_split=0.2, shuffle=True, seed=1234):\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(dataset)\n",
    "    train_size = int(len(dataset) * (1 - val_split))\n",
    "    train_dataset = dataset[:train_size]\n",
    "    val_dataset = dataset[train_size:]\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def train_model(params, models, dataset, path_save, save_frequency=5):  \n",
    "    opt = torch.optim.Adam(\n",
    "    list(models[0].parameters()) + list(models[1].parameters()),\n",
    "    lr=learning_rate,\n",
    "    )\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    loss_list = []\n",
    "    train_dataset, val_dataset = split_train_val(dataset, val_split=0.1)\n",
    "    for k in range(params[\"epochs\"]):\n",
    "        my_loss_list = []\n",
    "        print(\"=\"*10, f\"EPOCH #{k+1}\",\"=\"*10)\n",
    "        for l, batch in enumerate(list_batchs(train_dataset, params[\"batch_size\"])):\n",
    "            optimizer.zero_grad()\n",
    "            my_loss_list = []\n",
    "            for j, graph in enumerate(batch):\n",
    "                i = graph[\"A\"]\n",
    "                v_in = [rev_dist(e) for e in graph[\"edges_feature\"]]\n",
    "                v_true = graph[\"true_edges\"]\n",
    "                x = graph[\"nodes_feature\"]\n",
    "                N = len(x)\n",
    "                \n",
    "                X = torch.Tensor(x)\n",
    "                sp_A = torch.sparse_coo_tensor(i, v_in, (N, N))\n",
    "                E_true = torch.Tensor([v_true]).T\n",
    "            \n",
    "                H_end = node_gnn(X, sp_A)\n",
    "                Omega = torch.cat([H_end[i[0]], H_end[i[1]]],dim=1)\n",
    "                E_pred = edge_linear(Omega)\n",
    "                loss_ = criterion(E_pred, E_true)\n",
    "                print(loss_)\n",
    "                if j == 0:\n",
    "                    loss = loss_\n",
    "                else:\n",
    "                    loss += loss_\n",
    "                print(loss)\n",
    "                my_loss_list.append(loss_.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_list.append(np.mean(my_loss_list))\n",
    "        print(f\"\\nBatch # {l+1} loss={my_loss_list[-1]:.4f}\" + \" \"*40)\n",
    "        train_val = np.mean(my_loss_list)\n",
    "        validation_val = validation(model, val_dataset)\n",
    "        with open('log.txt', 'a') as f:\n",
    "            f.write(f\"EPOCH #{i}\\t {train_val:.4f} (VAL: {validation_val:.4f})\\n\")  \n",
    "        if i % save_frequency == 0:\n",
    "            model.save(path_save+f\"_{i//save_frequency}\")\n",
    "    model.save(path_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d2e2674a-fb87-4c50-a726-32d39a29bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== EPOCH #1 ==========\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n",
      "tensor(-0., grad_fn=<DivBackward1>)\n",
      "tensor(-0., grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_gnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_linear\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(params, models, dataset, path_save, save_frequency)\u001b[0m\n\u001b[1;32m     36\u001b[0m sp_A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_coo_tensor(i, v_in, (N, N))\n\u001b[1;32m     37\u001b[0m E_true \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([v_true])\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m---> 39\u001b[0m H_end \u001b[38;5;241m=\u001b[39m \u001b[43mnode_gnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp_A\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m Omega \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([H_end[i[\u001b[38;5;241m0\u001b[39m]], H_end[i[\u001b[38;5;241m1\u001b[39m]]],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m E_pred \u001b[38;5;241m=\u001b[39m edge_linear(Omega)\n",
      "File \u001b[0;32m~/project/PageR/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/PageR/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[59], line 9\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, edge_index: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m relu(x)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[0;32m~/project/PageR/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/PageR/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/project/PageR/env/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/project/PageR/env/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:89\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     86\u001b[0m col, row \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m], edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     88\u001b[0m deg \u001b[38;5;241m=\u001b[39m scatter(value, col, \u001b[38;5;241m0\u001b[39m, dim_size\u001b[38;5;241m=\u001b[39mnum_nodes, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m deg_inv_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mdeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m deg_inv_sqrt\u001b[38;5;241m.\u001b[39mmasked_fill_(deg_inv_sqrt \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     91\u001b[0m value \u001b[38;5;241m=\u001b[39m deg_inv_sqrt[row] \u001b[38;5;241m*\u001b[39m value \u001b[38;5;241m*\u001b[39m deg_inv_sqrt[col]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "train_model(params, [node_gnn, edge_linear], dataset, \"torch\", save_frequency=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d097ece-7450-4682-b01d-f178dc020a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
