{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58491147-580d-409e-aedf-2d49997dfea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch-geometric)\n",
      "  Using cached aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: fsspec in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (6.0.0)\n",
      "Requirement already satisfied: pyparsing in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: requests in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (2.32.3)\n",
      "Collecting tqdm (from torch-geometric)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->torch-geometric)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from aiohttp->torch-geometric) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch-geometric)\n",
      "  Using cached propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch-geometric)\n",
      "  Using cached yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from jinja2->torch-geometric) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (2024.8.30)\n",
      "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "Using cached aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Using cached propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Using cached yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Installing collected packages: tqdm, propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch-geometric\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 frozenlist-1.5.0 multidict-6.1.0 propcache-0.2.1 torch-geometric-2.6.1 tqdm-4.67.1 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f8ed93-0f7e-4ded-8b2a-73e2374eb7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import relu, sigmoid, binary_cross_entropy\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b56272de-cf3d-409c-87a2-ee7fcb82fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self,  layers):\n",
    "        super(GNN, self).__init__()\n",
    "        convs = []\n",
    "        Bs = []\n",
    "        for l_in, l_out in zip(layers[:-1], layers[1:]):\n",
    "            convs.append(GCNConv(l_in, l_out, bias=False))\n",
    "            torch.nn.init.normal_(convs[-1].lin.weight,mean=0.01, std=0.3)\n",
    "            Bs.append(torch.nn.Linear(l_in, l_out, bias=False))\n",
    "            torch.nn.init.normal_(Bs[-1].weight, mean=0.5, std=0.3)\n",
    "        self.convs = torch.nn.ModuleList(convs)\n",
    "        self.Bs = torch.nn.ModuleList(Bs)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        for conv, B in zip(self.convs, self.Bs):\n",
    "            x = conv(x, edge_index) -  B(x)\n",
    "            x = relu(x)\n",
    "        return x\n",
    "\n",
    "class EdgesMLP(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(EdgesMLP, self).__init__()\n",
    "        linears = []\n",
    "        for l_in, l_out in zip(layers[:-1], layers[1:]):\n",
    "            linears.append(Linear(l_in, l_out, bias=False))\n",
    "            torch.nn.init.normal_(linears[-1].weight, mean=0.5, std=0.3)\n",
    "        self.linears = linears\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "            x = sigmoid(x)\n",
    "        return torch.squeeze(x, 1)\n",
    "\n",
    "def get_models(params):\n",
    "    layers_gnn = params[\"count_neuron_layers_gnn\"]\n",
    "    layers_edge = params[\"count_neuron_layers_edge\"]\n",
    "    node_gnn = GNN(layers_gnn)\n",
    "    edge_linear = EdgesMLP(layers_edge)\n",
    "    return node_gnn, edge_linear\n",
    "\n",
    "def list_batchs(dataset, batch_size):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i:i+batch_size]\n",
    "\n",
    "def get_tensor_from_graph(graph):\n",
    "    i = graph[\"A\"]\n",
    "    v_in = [rev_dist(e) for e in graph[\"edges_feature\"]]\n",
    "    v_true = graph[\"true_edges\"]\n",
    "    x = graph[\"nodes_feature\"]\n",
    "    N = len(x)\n",
    "    \n",
    "    X = torch.tensor(data=x, dtype=torch.float32)\n",
    "    sp_A = torch.sparse_coo_tensor(indices=i, values=v_in, size=(N, N), dtype=torch.float32)\n",
    "    E_true = torch.tensor(data=v_true, dtype=torch.float32)\n",
    "    return X, sp_A, E_true, i\n",
    "\n",
    "def validation(models, dataset, criterion):\n",
    "    my_loss_list = []\n",
    "    for j, graph in enumerate(dataset):\n",
    "        X, sp_A, E_true, i = get_tensor_from_graph(graph)\n",
    "        H_end = models[0](X, sp_A)\n",
    "        Omega = torch.cat([H_end[i[0]], H_end[i[1]]],dim=1)\n",
    "        E_pred = models[1](Omega)\n",
    "        loss = criterion(E_pred, E_true)\n",
    "        my_loss_list.append(loss.item())\n",
    "        print(f\"{(j+1)/len(dataset)*100:.2f} % loss = {my_loss_list[-1]:.5f} {' '*30}\", end='\\r')\n",
    "    return np.mean(my_loss_list)\n",
    "\n",
    "def split_train_val(dataset, val_split=0.2, shuffle=True, seed=1234):\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(dataset)\n",
    "    train_size = int(len(dataset) * (1 - val_split))\n",
    "    train_dataset = dataset[:train_size]\n",
    "    val_dataset = dataset[train_size:]\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def train_step(models, batch, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    my_loss_list = []\n",
    "   \n",
    "    for j, graph in enumerate(batch):\n",
    "        X, sp_A, E_true, i = get_tensor_from_graph(graph)\n",
    "        H_end = models[0](X, sp_A)\n",
    "        Omega = torch.cat([H_end[i[0]], H_end[i[1]]],dim=1)\n",
    "        E_pred = models[1](Omega)\n",
    "        loss = criterion(E_pred, E_true)\n",
    "        my_loss_list.append(loss.item())\n",
    "        print(f\"Batch loss={my_loss_list[-1]:.4f}\" + \" \"*40, end=\"\\r\")\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    return np.mean(my_loss_list)\n",
    "\n",
    "def train_model(params, models, dataset, path_save, save_frequency=5, restart=False):  \n",
    "    optimizer = torch.optim.Adam(\n",
    "    list(models[0].parameters()) + list(models[1].parameters()),\n",
    "    lr=learning_rate,\n",
    "    )\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss_list = []\n",
    "    with open('log.txt', 'a') as f:\n",
    "        for key, val in params.items():\n",
    "            f.write(f\"{key}:\\t{val}\\n\")\n",
    "    train_dataset, val_dataset = split_train_val(dataset, val_split=0.1)\n",
    "    for k in range(params[\"epochs\"]):\n",
    "        my_loss_list = []\n",
    "        \n",
    "        for l, batch in enumerate(list_batchs(train_dataset, params[\"batch_size\"])):\n",
    "            batch_loss = train_step(models, batch, optimizer, criterion)\n",
    "            my_loss_list.append(batch_loss)\n",
    "            print(f\"Batch # {l+1} loss={my_loss_list[-1]:.4f}\" + \" \"*40)\n",
    "        train_val = np.mean(my_loss_list)\n",
    "        loss_list.append(train_val)\n",
    "        validation_val = validation(models, val_dataset, criterion)\n",
    "        print(\"=\"*10, f\"EPOCH #{k+1}\",\"=\"*10, f\"({train_val:.4f}/{validation_val:.4f})\")\n",
    "        \n",
    "        # TODO: DELETE RESTART\n",
    "        if restart and k>=2 and abs(loss_list[k] - loss_list[k-1]) < 0.001:\n",
    "            return True\n",
    "            \n",
    "            \n",
    "        with open('log.txt', 'a') as f:\n",
    "            f.write(f\"EPOCH #{k}\\t {train_val:.8f} (VAL: {validation_val:.8f})\\n\")  \n",
    "        if (k+1) % save_frequency == 0:\n",
    "            num = k//save_frequency\n",
    "            torch.save(models[0].state_dict(), path_save+f\"_node_gnn_{num}\")\n",
    "            torch.save(models[1].state_dict(), path_save+f\"_edge_linear_{num}\")\n",
    "    torch.save(models[0].state_dict(), path_save+f\"_node_gnn_end\")\n",
    "    torch.save(models[1].state_dict(), path_save+f\"_edge_linear_end\")\n",
    "    return False # For restart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d873f8d-d98c-4325-9aac-3cc68cb7b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET INFO:\n",
      "count row: 11900\n",
      "first: dict_keys(['A', 'nodes_feature', 'edges_feature', 'true_edges'])\n",
      "\t A: (2, 925)\n",
      "\t nodes_feature: (451, 9)\n",
      "\t edges_feature: (925,)\n",
      "\t true_edges: (925,)\n",
      "end: dict_keys(['A', 'nodes_feature', 'edges_feature', 'true_edges'])\n",
      "\t A: (2, 1597)\n",
      "\t nodes_feature: (778, 9)\n",
      "\t edges_feature: (1597,)\n",
      "\t true_edges: (1597,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# with open(\"../dataset.json\", \"r\") as f:\n",
    "#     dataset = json.load(f)['dataset']\n",
    "with open(\"/home/daniil/pager_11000_4N_seg.json\", \"r\") as f:\n",
    "    dataset = json.load(f)['dataset']\n",
    "\n",
    "print(\"DATASET INFO:\")\n",
    "print(\"count row:\", len(dataset))\n",
    "print(\"first:\", dataset[0].keys())\n",
    "print(f\"\\t A:\", np.shape(dataset[0][\"A\"]))\n",
    "print(f\"\\t nodes_feature:\", np.shape(dataset[0][\"nodes_feature\"]))\n",
    "print(f\"\\t edges_feature:\", np.shape(dataset[0][\"edges_feature\"]))\n",
    "print(f\"\\t true_edges:\", np.shape(dataset[0][\"true_edges\"]))\n",
    "print(\"end:\", dataset[-1].keys())\n",
    "print(f\"\\t A:\", np.shape(dataset[-1][\"A\"]))\n",
    "print(f\"\\t nodes_feature:\", np.shape(dataset[-1][\"nodes_feature\"]))\n",
    "print(f\"\\t edges_feature:\", np.shape(dataset[-1][\"edges_feature\"]))\n",
    "print(f\"\\t true_edges:\", np.shape(dataset[-1][\"true_edges\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5096a10-c1be-42c4-9a04-fc6d776ea7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_dist(a):\n",
    "    if a==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/a\n",
    "        \n",
    "i = dataset[0][\"A\"]\n",
    "v_in = [rev_dist(e) for e in dataset[0][\"edges_feature\"]]\n",
    "v_true = dataset[0][\"true_edges\"]\n",
    "x = dataset[0][\"nodes_feature\"]\n",
    "N = len(x)\n",
    "\n",
    "X = torch.Tensor(x)\n",
    "sp_A = torch.sparse_coo_tensor(i, v_in, (N, N))\n",
    "E_true = torch.Tensor(v_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84eb1c09-c6e5-4280-ac3e-b5763b89b46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_pred:\n",
      "tensor([0.8174, 0.8295, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8295, 0.8174,\n",
      "        0.8255, 0.8255, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8368, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8323, 0.8323, 0.8323, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8306, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8306, 0.8174,\n",
      "        0.8174, 0.8306, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8264,\n",
      "        0.8264, 0.8264, 0.8174, 0.8182, 0.8182, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8182, 0.8174, 0.8174, 0.8174, 0.8174, 0.8182, 0.8184,\n",
      "        0.8184, 0.8174, 0.8174, 0.8183, 0.8174, 0.8174, 0.8179, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8180, 0.8180, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8388, 0.8346, 0.8346,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8289, 0.8174,\n",
      "        0.8251, 0.8251, 0.8174, 0.8387, 0.8174, 0.8174, 0.8414, 0.8346, 0.8356,\n",
      "        0.8346, 0.8223, 0.8194, 0.8193, 0.8193, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8271, 0.8271,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8271, 0.8180, 0.8237, 0.8242, 0.8174,\n",
      "        0.8184, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8343, 0.8360, 0.8174, 0.8187, 0.8178, 0.8178, 0.8180,\n",
      "        0.8180, 0.8180, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8390, 0.8174, 0.8395, 0.8174, 0.8285, 0.8174, 0.8174, 0.8174, 0.8221,\n",
      "        0.8174, 0.8221, 0.8174, 0.8390, 0.8174, 0.8416, 0.8350, 0.8350, 0.8418,\n",
      "        0.8352, 0.8418, 0.8356, 0.8247, 0.8247, 0.8247, 0.8221, 0.8283, 0.8203,\n",
      "        0.8203, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8297, 0.8297, 0.8174, 0.8174, 0.8174, 0.8174, 0.8179, 0.8179,\n",
      "        0.8179, 0.8179, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8360,\n",
      "        0.8174, 0.8174, 0.8314, 0.8314, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8283, 0.8174, 0.8246,\n",
      "        0.8246, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8179,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8217, 0.8174, 0.8217, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8336, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8177, 0.8174, 0.8177, 0.8177,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8217,\n",
      "        0.8234, 0.8240, 0.8200, 0.8201, 0.8201, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8336,\n",
      "        0.8212, 0.8289, 0.8289, 0.8174, 0.8174, 0.8174, 0.8174, 0.8177, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8177, 0.8174, 0.8176, 0.8176, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8211, 0.8211, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8212, 0.8174, 0.8197,\n",
      "        0.8197, 0.8174, 0.8252, 0.8174, 0.8279, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8252, 0.8174, 0.8224, 0.8320, 0.8243, 0.8243,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8315, 0.8174, 0.8271,\n",
      "        0.8271, 0.8174, 0.8213, 0.8174, 0.8213, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8417, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8369, 0.8174, 0.8323, 0.8423, 0.8174,\n",
      "        0.8174, 0.8174, 0.8369, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8417, 0.8174, 0.8388, 0.8388, 0.8388, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8424, 0.8399, 0.8399, 0.8174, 0.8174, 0.8373, 0.8373, 0.8373, 0.8373,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8202, 0.8213,\n",
      "        0.8174, 0.8233, 0.8198, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8202, 0.8174, 0.8191, 0.8191, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174,\n",
      "        0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174, 0.8174],\n",
      "       grad_fn=<SqueezeBackward1>) \n",
      "E_true:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1.])\n",
      "Loss =  tensor(0.3267, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"count_neuron_layers_gnn\": [9, 18, 6],\n",
    "    \"count_neuron_layers_edge\": [6*2, 8, 4, 1],\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 1500,\n",
    "}\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "node_gnn, edge_linear = get_models(params)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(node_gnn.parameters()) + list(edge_linear.parameters()),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "H_end = node_gnn(X, sp_A)\n",
    "Omega = torch.cat([H_end[i[0]], H_end[i[1]]],dim=1)\n",
    "E_pred = edge_linear(Omega)\n",
    "print(f\"E_pred:\\n{E_pred}\", f\"\\nE_true:\\n{E_true}\")\n",
    "print(\"Loss = \", criterion(E_pred, E_true))\n",
    "\n",
    "del optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e60566c-0723-4b95-810b-594ac69c22f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=/=/=/=/=/=/=/=/=/=/NUMTEST 1 =/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/\n",
      "Batch # 1 loss=0.4047                                        \n",
      "Batch # 2 loss=0.4032                                        \n",
      "Batch # 3 loss=0.4022                                        \n",
      "Batch # 4 loss=0.4084                                        \n",
      "Batch # 5 loss=0.4005                                        \n",
      "Batch # 6 loss=0.4105                                        \n",
      "Batch # 7 loss=0.4077                                        \n",
      "Batch # 8 loss=0.4095                                        \n",
      "========== EPOCH #1 ========== (0.4058/0.4147)        \n",
      "Batch # 1 loss=0.4044                                        \n",
      "Batch # 2 loss=0.4029                                        \n",
      "Batch # 3 loss=0.4019                                        \n",
      "Batch # 4 loss=0.4084                                        \n",
      "Batch # 5 loss=0.4054                                        \n",
      "Batch # 6 loss=0.4102                                        \n",
      "Batch # 7 loss=0.4079                                        \n",
      "Batch # 8 loss=0.4099                                        \n",
      "========== EPOCH #2 ========== (0.4064/0.4149)        \n",
      "Batch # 1 loss=0.4046                                        \n",
      "Batch # 2 loss=0.4029                                        \n",
      "Batch # 3 loss=0.4019                                        \n",
      "Batch # 4 loss=0.4085                                        \n",
      "Batch # 5 loss=0.4000                                        \n",
      "Batch # 6 loss=0.4101                                        \n",
      "Batch # 7 loss=0.4077                                        \n",
      "Batch # 8 loss=0.4095                                        \n",
      "========== EPOCH #3 ========== (0.4057/0.4146)        \n",
      "=/=/=/=/=/=/=/=/=/=/NUMTEST 2 =/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/\n",
      "Batch # 1 loss=0.4093                                        \n",
      "Batch # 2 loss=0.4101                                        \n",
      "Batch # 3 loss=0.4094                                        \n",
      "Batch # 4 loss=0.4098                                        \n",
      "Batch # 5 loss=0.4058                                        \n",
      "Batch # 6 loss=0.4067                                        \n",
      "Batch # 7 loss=0.4040                                        \n",
      "Batch # 8 loss=0.3929                                        \n",
      "========== EPOCH #1 ========== (0.4060/0.4150)        \n",
      "Batch # 1 loss=0.4067                                        \n",
      "Batch # 2 loss=0.4087                                        \n",
      "Batch # 3 loss=0.4077                                        \n",
      "Batch # 4 loss=0.4080                                        \n",
      "Batch # 5 loss=0.4040                                        \n",
      "Batch # 6 loss=0.4053                                        \n",
      "Batch # 7 loss=0.4043                                        \n",
      "Batch # 8 loss=0.3929                                        \n",
      "========== EPOCH #2 ========== (0.4047/0.4143)        \n",
      "Batch # 1 loss=0.4061                                        \n",
      "Batch # 2 loss=0.4082                                        \n",
      "Batch # 3 loss=0.4075                                        \n",
      "Batch # 4 loss=0.4080                                        \n",
      "Batch # 5 loss=0.4039                                        \n",
      "Batch # 6 loss=0.4051                                        \n",
      "Batch # 7 loss=0.4041                                        \n",
      "Batch # 8 loss=0.3928                                        \n",
      "========== EPOCH #3 ========== (0.4044/0.4143)        \n",
      "=/=/=/=/=/=/=/=/=/=/NUMTEST 3 =/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/\n",
      "Batch # 1 loss=0.5075                                        \n",
      "Batch # 2 loss=0.5027                                        \n",
      "Batch # 3 loss=0.5030                                        \n",
      "Batch # 4 loss=0.4981                                        \n",
      "Batch # 5 loss=0.4950                                        \n",
      "Batch # 6 loss=0.4967                                        \n",
      "Batch # 7 loss=0.4910                                        \n",
      "Batch # 8 loss=0.4889                                        \n",
      "========== EPOCH #1 ========== (0.4979/0.4859)        \n",
      "Batch # 1 loss=0.4938                                        \n",
      "Batch # 2 loss=0.4890                                        \n",
      "Batch # 3 loss=0.4941                                        \n",
      "Batch # 4 loss=0.4925                                        \n",
      "Batch # 5 loss=0.4918                                        \n",
      "Batch # 6 loss=0.4949                                        \n",
      "Batch # 7 loss=0.4901                                        \n",
      "Batch # 8 loss=0.4887                                        \n",
      "========== EPOCH #2 ========== (0.4919/0.4861)        \n",
      "Batch # 1 loss=0.4940                                        \n",
      "Batch # 2 loss=0.4890                                        \n",
      "Batch # 3 loss=0.4941                                        \n",
      "Batch # 4 loss=0.4925                                        \n",
      "Batch # 5 loss=0.4914                                        \n",
      "Batch # 6 loss=0.4947                                        \n",
      "Batch # 7 loss=0.4899                                        \n",
      "Batch # 8 loss=0.4885                                        \n",
      "========== EPOCH #3 ========== (0.4918/0.4859)        \n",
      "=/=/=/=/=/=/=/=/=/=/NUMTEST 4 =/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/\n",
      "Batch # 1 loss=0.4088                                        \n",
      "Batch # 2 loss=0.4146                                        \n",
      "Batch # 3 loss=0.4125                                        \n",
      "Batch # 4 loss=0.4144                                        \n",
      "Batch # 5 loss=0.4142                                        \n",
      "Batch # 6 loss=0.4062                                        \n",
      "Batch # 7 loss=0.4082                                        \n",
      "Batch # 8 loss=0.4245                                        \n",
      "========== EPOCH #1 ========== (0.4129/0.4007)        \n",
      "Batch # 1 loss=0.4053                                        \n",
      "Batch # 2 loss=0.4108                                        \n",
      "Batch # 3 loss=0.4097                                        \n",
      "Batch # 4 loss=0.4127                                        \n",
      "Batch # 5 loss=0.4134                                        \n",
      "Batch # 6 loss=0.4059                                        \n",
      "Batch # 7 loss=0.4076                                        \n",
      "Batch # 8 loss=0.4241                                        \n",
      "========== EPOCH #2 ========== (0.4112/0.4004)        \n",
      "Batch # 1 loss=0.4050                                        \n",
      "Batch # 2 loss=0.4107                                        \n",
      "Batch # 3 loss=0.4097                                        \n",
      "Batch # 4 loss=0.4127                                        \n",
      "Batch # 5 loss=0.4133                                        \n",
      "Batch # 6 loss=0.4056                                        \n",
      "Batch # 7 loss=0.4076                                        \n",
      "Batch # 8 loss=0.4240                                        \n",
      "========== EPOCH #3 ========== (0.4111/0.4005)        \n",
      "=/=/=/=/=/=/=/=/=/=/NUMTEST 5 =/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/\n",
      "Batch # 1 loss=0.4362                                        \n",
      "Batch # 2 loss=0.4229                                        \n",
      "Batch # 3 loss=0.4292                                        \n",
      "Batch # 4 loss=0.4178                                        \n",
      "Batch loss=0.2443                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUMTEST \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(num_test) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      6\u001b[0m models \u001b[38;5;241m=\u001b[39m get_models(params)\n\u001b[0;32m----> 7\u001b[0m restart \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_test\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_deep_torch_11000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 111\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(params, models, dataset, path_save, save_frequency, restart)\u001b[0m\n\u001b[1;32m    108\u001b[0m my_loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(list_batchs(train_dataset, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[0;32m--> 111\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     my_loss_list\u001b[38;5;241m.\u001b[39mappend(batch_loss)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch # \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_loss_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 85\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(models, batch, optimizer, criterion)\u001b[0m\n\u001b[1;32m     82\u001b[0m my_loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, graph \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[0;32m---> 85\u001b[0m     X, sp_A, E_true, i \u001b[38;5;241m=\u001b[39m \u001b[43mget_tensor_from_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     H_end \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;241m0\u001b[39m](X, sp_A)\n\u001b[1;32m     87\u001b[0m     Omega \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([H_end[i[\u001b[38;5;241m0\u001b[39m]], H_end[i[\u001b[38;5;241m1\u001b[39m]]],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m, in \u001b[0;36mget_tensor_from_graph\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tensor_from_graph\u001b[39m(graph):\n\u001b[1;32m     48\u001b[0m     i \u001b[38;5;241m=\u001b[39m graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 49\u001b[0m     v_in \u001b[38;5;241m=\u001b[39m [rev_dist(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     50\u001b[0m     v_true \u001b[38;5;241m=\u001b[39m graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue_edges\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "restart = True\n",
    "num_test = 0\n",
    "while restart:\n",
    "    num_test += 1\n",
    "    print(\"=/\"*10 + \"NUMTEST \" + str(num_test) + \" \" + \"=/\"*30)\n",
    "    models = get_models(params)\n",
    "    restart = train_model(params, models, dataset, f\"{num_test}_deep_torch_11000\", save_frequency=5, restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2674a-fb87-4c50-a726-32d39a29bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weigths(models, path_node_gnn, path_edge_linear):\n",
    "    models[0].load_state_dict(torch.load(path_node_gnn, weights_only=True))\n",
    "    models[1].load_state_dict(torch.load(path_edge_linear, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca54f343-cb1a-4731-9c2c-c0d71d48c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_load = get_models(params)\n",
    "load_weigths(models_load, \"deep_torch_11000_node_gnn_end\", \"deep_torch_11000_edge_linear_end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d097ece-7450-4682-b01d-f178dc020a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_classification_edges(models, graph, k=0.51):\n",
    "    i = graph[\"A\"]\n",
    "    v_in = [rev_dist(e) for e in graph[\"edges_feature\"]]\n",
    "    x = graph[\"nodes_feature\"]\n",
    "    N = len(x)\n",
    "    X = torch.tensor(data=x, dtype=torch.float32)\n",
    "    sp_A = torch.sparse_coo_tensor(indices=i, values=v_in, size=(N, N), dtype=torch.float32)\n",
    "    \n",
    "    H_end = models[0](X, sp_A)\n",
    "    Omega = torch.cat([H_end[i[0]], H_end[i[1]]],dim=1)\n",
    "    E_pred = models[1](Omega)\n",
    "    a = np.zeros(E_pred.shape)\n",
    "    return E_pred\n",
    "    a[E_pred>k] = 1.0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecfdfcd6-00c4-4813-bb17-29d5a86bd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch_classification_edges(models, dataset[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bac399b2-9eee-4986-992a-6fc2b542b441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mask == np.array(dataset[2]['true_edges']))/ len(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ca165-54bb-435f-87e6-436d5935df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.90 - для 1500 изображений\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7192cb90-9f67-4d5f-816a-32883794b407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in dataset[2]['true_edges']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f7eb3-f353-43c8-9ccc-b75d80e6551c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
