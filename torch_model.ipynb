{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58491147-580d-409e-aedf-2d49997dfea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch-geometric)\n",
      "  Using cached aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: fsspec in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (6.0.0)\n",
      "Requirement already satisfied: pyparsing in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: requests in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from torch-geometric) (2.32.3)\n",
      "Collecting tqdm (from torch-geometric)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->torch-geometric)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from aiohttp->torch-geometric) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch-geometric)\n",
      "  Using cached propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch-geometric)\n",
      "  Using cached yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from jinja2->torch-geometric) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/daniil/project/PageR/env/lib/python3.12/site-packages (from requests->torch-geometric) (2024.8.30)\n",
      "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "Using cached aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Using cached propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Using cached yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Installing collected packages: tqdm, propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch-geometric\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 frozenlist-1.5.0 multidict-6.1.0 propcache-0.2.1 torch-geometric-2.6.1 tqdm-4.67.1 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f8ed93-0f7e-4ded-8b2a-73e2374eb7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/project/PageR/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import relu, sigmoid, binary_cross_entropy\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b56272de-cf3d-409c-87a2-ee7fcb82fccd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self,  layers):\n",
    "        super(GNN, self).__init__()\n",
    "        convs = []\n",
    "        Bs = []\n",
    "        for l_in, l_out in zip(layers[:-1], layers[1:]):\n",
    "            convs.append(GCNConv(l_in, l_out, bias=False))\n",
    "            torch.nn.init.normal_(convs[-1].lin.weight,mean=0.01, std=0.3)\n",
    "            Bs.append(torch.nn.Linear(l_in, l_out, bias=False))\n",
    "            torch.nn.init.normal_(Bs[-1].weight, mean=0.5, std=0.3)\n",
    "        self.convs = torch.nn.ModuleList(convs)\n",
    "        self.Bs = torch.nn.ModuleList(Bs)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        for conv, B in zip(self.convs, self.Bs):\n",
    "            x = conv(x, edge_index) -  B(x)\n",
    "            x = relu(x)\n",
    "        return x\n",
    "\n",
    "class EdgesMLP(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(EdgesMLP, self).__init__()\n",
    "        linears = []\n",
    "        for l_in, l_out in zip(layers[:-1], layers[1:]):\n",
    "            linears.append(Linear(l_in, l_out, bias=False))\n",
    "            torch.nn.init.normal_(linears[-1].weight, mean=0.5, std=0.3)\n",
    "        self.linears = linears\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "            x = sigmoid(x)\n",
    "        return torch.squeeze(x, 1)\n",
    "\n",
    "def get_models(params):\n",
    "    layers_gnn = params[\"count_neuron_layers_gnn\"]\n",
    "    layers_edge = params[\"count_neuron_layers_edge\"]\n",
    "    node_gnn = GNN(layers_gnn)\n",
    "    edge_linear = EdgesMLP(layers_edge)\n",
    "    return node_gnn, edge_linear\n",
    "\n",
    "def list_batchs(dataset, batch_size):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i:i+batch_size]\n",
    "\n",
    "def get_tensor_from_graph(graph):\n",
    "    i = graph[\"A\"]\n",
    "    v_in = [rev_dist(e) for e in graph[\"edges_feature\"]]\n",
    "    v_true = graph[\"true_edges\"]\n",
    "    x = graph[\"nodes_feature\"]\n",
    "    N = len(x)\n",
    "    \n",
    "    X = torch.tensor(data=x, dtype=torch.float32)\n",
    "    sp_A = torch.sparse_coo_tensor(indices=i, values=v_in, size=(N, N), dtype=torch.float32)\n",
    "    E_true = torch.tensor(data=v_true, dtype=torch.float32)\n",
    "    return X, sp_A, E_true, i\n",
    "\n",
    "def validation(models, dataset, criterion):\n",
    "    my_loss_list = []\n",
    "    for j, graph in enumerate(dataset):\n",
    "        X, sp_A, E_true, i = get_tensor_from_graph(graph)\n",
    "        H_end = models[0](X, sp_A)\n",
    "        Omega = torch.cat([H_end[i[0]], H_end[i[1]]],dim=1)\n",
    "        E_pred = models[1](Omega)\n",
    "        loss = criterion(E_pred, E_true)\n",
    "        my_loss_list.append(loss.item())\n",
    "        print(f\"{(j+1)/len(dataset)*100:.2f} % loss = {my_loss_list[-1]:.5f} {' '*30}\", end='\\r')\n",
    "    return np.mean(my_loss_list)\n",
    "\n",
    "def split_train_val(dataset, val_split=0.2, shuffle=True, seed=1234):\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(dataset)\n",
    "    train_size = int(len(dataset) * (1 - val_split))\n",
    "    train_dataset = dataset[:train_size]\n",
    "    val_dataset = dataset[train_size:]\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def train_step(models, batch, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    my_loss_list = []\n",
    "   \n",
    "    for j, graph in enumerate(batch):\n",
    "        X, sp_A, E_true, i = get_tensor_from_graph(graph)\n",
    "        H_end = models[0](X, sp_A)\n",
    "        Omega = torch.cat([H_end[i[0]], H_end[i[1]]],dim=1)\n",
    "        E_pred = models[1](Omega)\n",
    "        loss = criterion(E_pred, E_true)\n",
    "        my_loss_list.append(loss.item())\n",
    "        print(f\"Batch loss={my_loss_list[-1]:.4f}\" + \" \"*40, end=\"\\r\")\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    return np.mean(my_loss_list)\n",
    "\n",
    "def train_model(params, models, dataset, path_save, save_frequency=5, restart=False):  \n",
    "    optimizer = torch.optim.Adam(\n",
    "    list(models[0].parameters()) + list(models[1].parameters()),\n",
    "    lr=learning_rate,\n",
    "    )\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss_list = []\n",
    "    with open('log.txt', 'a') as f:\n",
    "        for key, val in params.items():\n",
    "            f.write(f\"{key}:\\t{val}\\n\")\n",
    "    train_dataset, val_dataset = split_train_val(dataset, val_split=0.1)\n",
    "    for k in range(params[\"epochs\"]):\n",
    "        my_loss_list = []\n",
    "        \n",
    "        for l, batch in enumerate(list_batchs(train_dataset, params[\"batch_size\"])):\n",
    "            batch_loss = train_step(models, batch, optimizer, criterion)\n",
    "            my_loss_list.append(batch_loss)\n",
    "            print(f\"Batch # {l+1} loss={my_loss_list[-1]:.4f}\" + \" \"*40)\n",
    "        train_val = np.mean(my_loss_list)\n",
    "        loss_list.append(train_val)\n",
    "        validation_val = validation(models, val_dataset, criterion)\n",
    "        print(\"=\"*10, f\"EPOCH #{k+1}\",\"=\"*10, f\"({train_val:.4f}/{validation_val:.4f})\")\n",
    "        \n",
    "        # TODO: DELETE RESTART\n",
    "        if restart and k>=2 and abs(loss_list[k] - loss_list[k-1]) < 0.001:\n",
    "            return True\n",
    "            \n",
    "            \n",
    "        with open('log.txt', 'a') as f:\n",
    "            f.write(f\"EPOCH #{k}\\t {train_val:.8f} (VAL: {validation_val:.8f})\\n\")  \n",
    "        if (k+1) % save_frequency == 0:\n",
    "            num = k//save_frequency\n",
    "            torch.save(models[0].state_dict(), path_save+f\"_node_gnn_{num}\")\n",
    "            torch.save(models[1].state_dict(), path_save+f\"_edge_linear_{num}\")\n",
    "    torch.save(models[0].state_dict(), path_save+f\"_node_gnn_end\")\n",
    "    torch.save(models[1].state_dict(), path_save+f\"_edge_linear_end\")\n",
    "    return False # For restart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d873f8d-d98c-4325-9aac-3cc68cb7b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET INFO:\n",
      "count row: 11900\n",
      "first: dict_keys(['A', 'nodes_feature', 'edges_feature', 'true_edges'])\n",
      "\t A: (2, 925)\n",
      "\t nodes_feature: (451, 9)\n",
      "\t edges_feature: (925,)\n",
      "\t true_edges: (925,)\n",
      "end: dict_keys(['A', 'nodes_feature', 'edges_feature', 'true_edges'])\n",
      "\t A: (2, 1597)\n",
      "\t nodes_feature: (778, 9)\n",
      "\t edges_feature: (1597,)\n",
      "\t true_edges: (1597,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# with open(\"../dataset.json\", \"r\") as f:\n",
    "#     dataset = json.load(f)['dataset']\n",
    "with open(\"/home/daniil/pager_11000_4N_seg.json\", \"r\") as f:\n",
    "    dataset = json.load(f)['dataset']\n",
    "\n",
    "print(\"DATASET INFO:\")\n",
    "print(\"count row:\", len(dataset))\n",
    "print(\"first:\", dataset[0].keys())\n",
    "print(f\"\\t A:\", np.shape(dataset[0][\"A\"]))\n",
    "print(f\"\\t nodes_feature:\", np.shape(dataset[0][\"nodes_feature\"]))\n",
    "print(f\"\\t edges_feature:\", np.shape(dataset[0][\"edges_feature\"]))\n",
    "print(f\"\\t true_edges:\", np.shape(dataset[0][\"true_edges\"]))\n",
    "print(\"end:\", dataset[-1].keys())\n",
    "print(f\"\\t A:\", np.shape(dataset[-1][\"A\"]))\n",
    "print(f\"\\t nodes_feature:\", np.shape(dataset[-1][\"nodes_feature\"]))\n",
    "print(f\"\\t edges_feature:\", np.shape(dataset[-1][\"edges_feature\"]))\n",
    "print(f\"\\t true_edges:\", np.shape(dataset[-1][\"true_edges\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5096a10-c1be-42c4-9a04-fc6d776ea7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_dist(a):\n",
    "    if a==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/a\n",
    "        \n",
    "i = dataset[0][\"A\"]\n",
    "v_in = [rev_dist(e) for e in dataset[0][\"edges_feature\"]]\n",
    "v_true = dataset[0][\"true_edges\"]\n",
    "x = dataset[0][\"nodes_feature\"]\n",
    "N = len(x)\n",
    "\n",
    "X = torch.Tensor(x)\n",
    "sp_A = torch.sparse_coo_tensor(i, v_in, (N, N))\n",
    "E_true = torch.Tensor(v_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84eb1c09-c6e5-4280-ac3e-b5763b89b46c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount_neuron_layers_gnn\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m27\u001b[39m, \u001b[38;5;241m18\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount_neuron_layers_edge\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m18\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[0;32m---> 10\u001b[0m node_gnn, edge_linear \u001b[38;5;241m=\u001b[39m \u001b[43mget_models\u001b[49m(params)\n\u001b[1;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mlist\u001b[39m(node_gnn\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(edge_linear\u001b[38;5;241m.\u001b[39mparameters()),\n\u001b[1;32m     15\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_models' is not defined"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"count_neuron_layers_gnn\": [9, 27, 18],\n",
    "    \"count_neuron_layers_edge\": [18*2, 1],\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 400,\n",
    "}\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "node_gnn, edge_linear = get_models(params)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(node_gnn.parameters()) + list(edge_linear.parameters()),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "H_end = node_gnn(X, sp_A)\n",
    "Omega = torch.cat([H_end[i[0]], H_end[i[1]]],dim=1)\n",
    "E_pred = edge_linear(Omega)\n",
    "print(f\"E_pred:\\n{E_pred}\", f\"\\nE_true:\\n{E_true}\")\n",
    "print(\"Loss = \", criterion(E_pred, E_true))\n",
    "\n",
    "del optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e60566c-0723-4b95-810b-594ac69c22f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 1 loss=1.7818                                        \n",
      "Batch # 2 loss=4.8147                                        \n",
      "Batch # 3 loss=2.3238                                        \n",
      "Batch # 4 loss=0.6592                                        \n",
      "Batch # 5 loss=0.6754                                        \n",
      "Batch # 6 loss=0.6888                                        \n",
      "Batch # 7 loss=0.6781                                        \n",
      "Batch # 8 loss=0.6668                                        \n",
      "Batch # 9 loss=0.6686                                        \n",
      "Batch # 10 loss=0.6430                                        \n",
      "Batch # 11 loss=0.6633                                        \n",
      "Batch # 12 loss=0.6160                                        \n",
      "Batch # 13 loss=0.6769                                        \n",
      "Batch # 14 loss=0.6758                                        \n",
      "Batch # 15 loss=0.6768                                        \n",
      "Batch # 16 loss=0.6778                                        \n",
      "Batch # 17 loss=0.6805                                        \n",
      "Batch # 18 loss=0.6806                                        \n",
      "Batch # 19 loss=0.6750                                        \n",
      "Batch # 20 loss=0.6760                                        \n",
      "Batch # 21 loss=0.6799                                        \n",
      "Batch # 22 loss=0.6869                                        \n",
      "Batch # 23 loss=0.6764                                        \n",
      "Batch # 24 loss=0.6702                                        \n",
      "Batch # 25 loss=1.3400                                        \n",
      "Batch # 26 loss=0.6564                                        \n",
      "Batch # 27 loss=0.6720                                        \n",
      "========== EPOCH #1 ========== (0.9511/0.6768)        \n",
      "Batch # 1 loss=0.6774                                        \n",
      "Batch # 2 loss=0.6747                                        \n",
      "Batch # 3 loss=0.6757                                        \n",
      "Batch # 4 loss=0.6751                                        \n",
      "Batch # 5 loss=0.6789                                        \n",
      "Batch # 6 loss=0.6811                                        \n",
      "Batch # 7 loss=0.6827                                        \n",
      "Batch # 8 loss=0.6850                                        \n",
      "Batch # 9 loss=0.6840                                        \n",
      "Batch # 10 loss=0.6840                                        \n",
      "Batch # 11 loss=0.6842                                        \n",
      "Batch # 12 loss=0.6850                                        \n",
      "Batch # 13 loss=0.6846                                        \n",
      "Batch # 14 loss=0.6851                                        \n",
      "Batch # 15 loss=0.6835                                        \n",
      "Batch # 16 loss=0.6831                                        \n",
      "Batch # 17 loss=0.6861                                        \n",
      "Batch # 18 loss=0.6815                                        \n",
      "Batch # 19 loss=0.6790                                        \n",
      "Batch # 20 loss=0.6066                                        \n",
      "Batch # 21 loss=0.6150                                        \n",
      "Batch # 22 loss=0.7357                                        \n",
      "Batch # 23 loss=0.6159                                        \n",
      "Batch # 24 loss=0.6488                                        \n",
      "Batch # 25 loss=0.6680                                        \n",
      "Batch # 26 loss=0.6784                                        \n",
      "Batch # 27 loss=0.6740                                        \n",
      "========== EPOCH #2 ========== (0.6738/0.6746)        \n",
      "Batch # 1 loss=0.6743                                        \n",
      "Batch # 2 loss=0.6743                                        \n",
      "Batch # 3 loss=0.6750                                        \n",
      "Batch # 4 loss=0.6741                                        \n",
      "Batch # 5 loss=0.6747                                        \n",
      "Batch # 6 loss=0.6743                                        \n",
      "Batch # 7 loss=0.6764                                        \n",
      "Batch # 8 loss=0.6844                                        \n",
      "Batch # 9 loss=0.6770                                        \n",
      "Batch # 10 loss=0.6749                                        \n",
      "Batch # 11 loss=0.6745                                        \n",
      "Batch # 12 loss=0.6767                                        \n",
      "Batch # 13 loss=0.6747                                        \n",
      "Batch # 14 loss=0.6758                                        \n",
      "Batch # 15 loss=0.6768                                        \n",
      "Batch # 16 loss=0.6743                                        \n",
      "Batch # 17 loss=0.6748                                        \n",
      "Batch # 18 loss=0.6762                                        \n",
      "Batch # 19 loss=0.6724                                        \n",
      "Batch # 20 loss=0.6704                                        \n",
      "Batch # 21 loss=0.6648                                        \n",
      "Batch # 22 loss=0.6081                                        \n",
      "Batch # 23 loss=0.6563                                        \n",
      "Batch # 24 loss=0.6667                                        \n",
      "Batch # 25 loss=0.6715                                        \n",
      "Batch # 26 loss=0.6816                                        \n",
      "Batch # 27 loss=0.6740                                        \n",
      "========== EPOCH #3 ========== (0.6714/0.6733)        \n",
      "Batch # 1 loss=0.6741                                        \n",
      "Batch # 2 loss=0.6729                                        \n",
      "Batch # 3 loss=0.6750                                        \n",
      "Batch # 4 loss=0.6743                                        \n",
      "Batch # 5 loss=0.6755                                        \n",
      "Batch # 6 loss=0.6745                                        \n",
      "Batch # 7 loss=0.6787                                        \n",
      "Batch # 8 loss=0.6816                                        \n",
      "Batch # 9 loss=0.6750                                        \n",
      "Batch # 10 loss=0.6718                                        \n",
      "Batch # 11 loss=0.6717                                        \n",
      "Batch # 12 loss=0.6701                                        \n",
      "Batch # 13 loss=0.6365                                        \n",
      "Batch # 14 loss=0.5404                                        \n",
      "Batch # 15 loss=0.6097                                        \n",
      "Batch # 16 loss=0.5070                                        \n",
      "Batch # 17 loss=0.5889                                        \n",
      "Batch # 18 loss=0.4922                                        \n",
      "Batch # 19 loss=0.7065                                        \n",
      "Batch # 20 loss=0.4490                                        \n",
      "Batch # 21 loss=0.5845                                        \n",
      "Batch # 22 loss=0.6280                                        \n",
      "Batch # 23 loss=0.6084                                        \n",
      "Batch # 24 loss=0.4872                                        \n",
      "Batch # 25 loss=0.5814                                        \n",
      "Batch # 26 loss=0.4921                                        \n",
      "Batch # 27 loss=0.5090                                        \n",
      "========== EPOCH #4 ========== (0.6117/0.5738)        \n",
      "Batch # 1 loss=0.5774                                        \n",
      "Batch # 2 loss=0.4611                                        \n",
      "Batch # 3 loss=0.4857                                        \n",
      "Batch # 4 loss=0.4750                                        \n",
      "Batch # 5 loss=0.5435                                        \n",
      "Batch # 6 loss=0.4957                                        \n",
      "Batch # 7 loss=0.5761                                        \n",
      "Batch # 8 loss=0.5545                                        \n",
      "Batch # 9 loss=0.4565                                        \n",
      "Batch # 10 loss=0.4433                                        \n",
      "Batch # 11 loss=0.4637                                        \n",
      "Batch # 12 loss=0.4303                                        \n",
      "Batch # 13 loss=0.4493                                        \n",
      "Batch # 14 loss=0.4264                                        \n",
      "Batch # 15 loss=0.4436                                        \n",
      "Batch # 16 loss=0.4250                                        \n",
      "Batch # 17 loss=0.4365                                        \n",
      "Batch # 18 loss=0.4586                                        \n",
      "Batch # 19 loss=0.4116                                        \n",
      "Batch # 20 loss=0.4261                                        \n",
      "Batch # 21 loss=0.4197                                        \n",
      "Batch # 22 loss=0.4396                                        \n",
      "Batch # 23 loss=0.4454                                        \n",
      "Batch # 24 loss=0.4237                                        \n",
      "Batch # 25 loss=0.4325                                        \n",
      "Batch # 26 loss=0.4140                                        \n",
      "Batch # 27 loss=0.4424                                        \n",
      "========== EPOCH #5 ========== (0.4614/0.4188)        \n",
      "Batch # 1 loss=0.4316                                        \n",
      "Batch # 2 loss=0.4203                                        \n",
      "Batch # 3 loss=0.4198                                        \n",
      "Batch # 4 loss=0.4035                                        \n",
      "Batch # 5 loss=0.4311                                        \n",
      "Batch # 6 loss=0.4138                                        \n",
      "Batch # 7 loss=0.4251                                        \n",
      "Batch # 8 loss=0.4099                                        \n",
      "Batch # 9 loss=0.4385                                        \n",
      "Batch # 10 loss=0.4016                                        \n",
      "Batch # 11 loss=0.4071                                        \n",
      "Batch # 12 loss=0.4231                                        \n",
      "Batch # 13 loss=0.4025                                        \n",
      "Batch # 14 loss=0.4183                                        \n",
      "Batch # 15 loss=0.4057                                        \n",
      "Batch # 16 loss=0.4005                                        \n",
      "Batch # 17 loss=0.4161                                        \n",
      "Batch # 18 loss=0.4245                                        \n",
      "Batch # 19 loss=0.4041                                        \n",
      "Batch # 20 loss=0.3946                                        \n",
      "Batch # 21 loss=0.4142                                        \n",
      "Batch # 22 loss=0.4179                                        \n",
      "Batch # 23 loss=0.4296                                        \n",
      "Batch # 24 loss=0.4092                                        \n",
      "Batch # 25 loss=0.4162                                        \n",
      "Batch # 26 loss=0.4039                                        \n",
      "Batch # 27 loss=0.4205                                        \n",
      "========== EPOCH #6 ========== (0.4149/0.4060)        \n",
      "Batch # 1 loss=0.4153                                        \n",
      "Batch # 2 loss=0.4102                                        \n",
      "Batch # 3 loss=0.3959                                        \n",
      "Batch # 4 loss=0.3993                                        \n",
      "Batch # 5 loss=0.4061                                        \n",
      "Batch # 6 loss=0.4053                                        \n",
      "Batch # 7 loss=0.4084                                        \n",
      "Batch # 8 loss=0.3941                                        \n",
      "Batch # 9 loss=0.4286                                        \n",
      "Batch # 10 loss=0.3992                                        \n",
      "Batch # 11 loss=0.4007                                        \n",
      "Batch # 12 loss=0.4125                                        \n",
      "Batch # 13 loss=0.4138                                        \n",
      "Batch # 14 loss=0.4362                                        \n",
      "Batch # 15 loss=0.3982                                        \n",
      "Batch # 16 loss=0.4287                                        \n",
      "Batch # 17 loss=0.4605                                        \n",
      "Batch # 18 loss=0.4386                                        \n",
      "Batch # 19 loss=0.4483                                        \n",
      "Batch # 20 loss=0.3997                                        \n",
      "Batch # 21 loss=0.4343                                        \n",
      "Batch # 22 loss=0.4438                                        \n",
      "Batch # 23 loss=0.4339                                        \n",
      "Batch # 24 loss=0.4543                                        \n",
      "Batch # 25 loss=0.4214                                        \n",
      "Batch # 26 loss=0.4202                                        \n",
      "Batch # 27 loss=0.4513                                        \n",
      "========== EPOCH #7 ========== (0.4207/0.4036)        \n",
      "Batch # 1 loss=0.4140                                        \n",
      "Batch # 2 loss=0.4376                                        \n",
      "Batch # 3 loss=0.3999                                        \n",
      "Batch # 4 loss=0.4173                                        \n",
      "Batch # 5 loss=0.4209                                        \n",
      "Batch # 6 loss=0.4155                                        \n",
      "Batch # 7 loss=0.4193                                        \n",
      "Batch # 8 loss=0.3986                                        \n",
      "Batch # 9 loss=0.4413                                        \n",
      "Batch # 10 loss=0.3973                                        \n",
      "Batch # 11 loss=0.4109                                        \n",
      "Batch # 12 loss=0.4190                                        \n",
      "Batch # 13 loss=0.4010                                        \n",
      "Batch # 14 loss=0.4051                                        \n",
      "Batch # 15 loss=0.4068                                        \n",
      "Batch # 16 loss=0.3874                                        \n",
      "Batch # 17 loss=0.4193                                        \n",
      "Batch # 18 loss=0.4126                                        \n",
      "Batch # 19 loss=0.4117                                        \n",
      "Batch # 20 loss=0.3868                                        \n",
      "Batch # 21 loss=0.4227                                        \n",
      "Batch # 22 loss=0.4107                                        \n",
      "Batch # 23 loss=0.4376                                        \n",
      "Batch # 24 loss=0.4025                                        \n",
      "Batch # 25 loss=0.4187                                        \n",
      "Batch # 26 loss=0.3996                                        \n",
      "Batch # 27 loss=0.4211                                        \n",
      "========== EPOCH #8 ========== (0.4124/0.4088)        \n",
      "Batch # 1 loss=0.4171                                        \n",
      "Batch # 2 loss=0.4074                                        \n",
      "Batch # 3 loss=0.3990                                        \n",
      "Batch # 4 loss=0.3933                                        \n",
      "Batch # 5 loss=0.4077                                        \n",
      "Batch # 6 loss=0.4008                                        \n",
      "Batch # 7 loss=0.4110                                        \n",
      "Batch # 8 loss=0.3930                                        \n",
      "Batch # 9 loss=0.4140                                        \n",
      "Batch # 10 loss=0.3869                                        \n",
      "Batch # 11 loss=0.3982                                        \n",
      "Batch # 12 loss=0.4071                                        \n",
      "Batch # 13 loss=0.3981                                        \n",
      "Batch # 14 loss=0.4086                                        \n",
      "Batch # 15 loss=0.3946                                        \n",
      "Batch # 16 loss=0.3948                                        \n",
      "Batch # 17 loss=0.4147                                        \n",
      "Batch # 18 loss=0.4092                                        \n",
      "Batch # 19 loss=0.4196                                        \n",
      "Batch # 20 loss=0.4105                                        \n",
      "Batch # 21 loss=0.4215                                        \n",
      "Batch # 22 loss=0.4483                                        \n",
      "Batch # 23 loss=0.4161                                        \n",
      "Batch # 24 loss=0.4143                                        \n",
      "Batch # 25 loss=0.4013                                        \n",
      "Batch # 26 loss=0.4107                                        \n",
      "Batch # 27 loss=0.4141                                        \n",
      "========== EPOCH #9 ========== (0.4078/0.4030)        \n",
      "Batch # 1 loss=0.4150                                        \n",
      "Batch # 2 loss=0.4043                                        \n",
      "Batch # 3 loss=0.3964                                        \n",
      "Batch # 4 loss=0.3944                                        \n",
      "Batch # 5 loss=0.4031                                        \n",
      "Batch # 6 loss=0.4015                                        \n",
      "Batch # 7 loss=0.4052                                        \n",
      "Batch # 8 loss=0.3954                                        \n",
      "Batch # 9 loss=0.4160                                        \n",
      "Batch # 10 loss=0.3865                                        \n",
      "Batch # 11 loss=0.3943                                        \n",
      "Batch # 12 loss=0.4050                                        \n",
      "Batch # 13 loss=0.3890                                        \n",
      "Batch # 14 loss=0.4003                                        \n",
      "Batch # 15 loss=0.3938                                        \n",
      "Batch # 16 loss=0.3808                                        \n",
      "Batch # 17 loss=0.4030                                        \n",
      "Batch # 18 loss=0.4125                                        \n",
      "Batch # 19 loss=0.3887                                        \n",
      "Batch # 20 loss=0.3794                                        \n",
      "Batch # 21 loss=0.3983                                        \n",
      "Batch # 22 loss=0.4129                                        \n",
      "Batch # 23 loss=0.4221                                        \n",
      "Batch # 24 loss=0.4122                                        \n",
      "Batch # 25 loss=0.4019                                        \n",
      "Batch # 26 loss=0.3912                                        \n",
      "Batch # 27 loss=0.4077                                        \n",
      "========== EPOCH #10 ========== (0.4004/0.4034)       \n",
      "Batch # 1 loss=0.4113                                        \n",
      "Batch # 2 loss=0.4262                                        \n",
      "Batch # 3 loss=0.3849                                        \n",
      "Batch # 4 loss=0.3913                                        \n",
      "Batch # 5 loss=0.4200                                        \n",
      "Batch # 6 loss=0.3911                                        \n",
      "Batch # 7 loss=0.4211                                        \n",
      "Batch # 8 loss=0.4104                                        \n",
      "Batch # 9 loss=0.4279                                        \n",
      "Batch # 10 loss=0.4234                                        \n",
      "Batch # 11 loss=0.3953                                        \n",
      "Batch # 12 loss=0.4275                                        \n",
      "Batch # 13 loss=0.3914                                        \n",
      "Batch # 14 loss=0.4053                                        \n",
      "Batch # 15 loss=0.4054                                        \n",
      "Batch # 16 loss=0.3836                                        \n",
      "Batch # 17 loss=0.4137                                        \n",
      "Batch # 18 loss=0.4147                                        \n",
      "Batch # 19 loss=0.3975                                        \n",
      "Batch # 20 loss=0.3915                                        \n",
      "Batch # 21 loss=0.4018                                        \n",
      "Batch # 22 loss=0.4139                                        \n",
      "Batch # 23 loss=0.4104                                        \n",
      "Batch # 24 loss=0.4103                                        \n",
      "Batch # 25 loss=0.3948                                        \n",
      "Batch # 26 loss=0.3981                                        \n",
      "Batch # 27 loss=0.4074                                        \n",
      "========== EPOCH #11 ========== (0.4063/0.4071)       \n",
      "Batch # 1 loss=0.4143                                        \n",
      "Batch # 2 loss=0.3920                                        \n",
      "Batch # 3 loss=0.3909                                        \n",
      "Batch # 4 loss=0.3818                                        \n",
      "Batch # 5 loss=0.4007                                        \n",
      "Batch # 6 loss=0.3907                                        \n",
      "Batch # 7 loss=0.3991                                        \n",
      "Batch # 8 loss=0.3868                                        \n",
      "Batch # 9 loss=0.4018                                        \n",
      "Batch # 10 loss=0.3758                                        \n",
      "Batch # 11 loss=0.3837                                        \n",
      "Batch # 12 loss=0.3952                                        \n",
      "Batch # 13 loss=0.3797                                        \n",
      "Batch # 14 loss=0.3907                                        \n",
      "Batch # 15 loss=0.3837                                        \n",
      "Batch # 16 loss=0.3724                                        \n",
      "Batch # 17 loss=0.3916                                        \n",
      "Batch # 18 loss=0.4042                                        \n",
      "Batch # 19 loss=0.3908                                        \n",
      "Batch # 20 loss=0.3720                                        \n",
      "Batch # 21 loss=0.3868                                        \n",
      "Batch # 22 loss=0.3960                                        \n",
      "Batch # 23 loss=0.4103                                        \n",
      "Batch # 24 loss=0.4071                                        \n",
      "Batch # 25 loss=0.3993                                        \n",
      "Batch # 26 loss=0.3929                                        \n",
      "Batch # 27 loss=0.4005                                        \n",
      "========== EPOCH #12 ========== (0.3923/0.4017)       \n",
      "Batch # 1 loss=0.4093                                        \n",
      "Batch # 2 loss=0.4470                                        \n",
      "Batch # 3 loss=0.3761                                        \n",
      "Batch # 4 loss=0.4521                                        \n",
      "Batch # 5 loss=0.4614                                        \n",
      "Batch # 6 loss=0.4508                                        \n",
      "Batch # 7 loss=0.4057                                        \n",
      "Batch # 8 loss=0.4676                                        \n",
      "Batch # 9 loss=0.4213                                        \n",
      "Batch # 10 loss=0.4055                                        \n",
      "Batch # 11 loss=0.4474                                        \n",
      "Batch # 12 loss=0.4179                                        \n",
      "Batch # 13 loss=0.4401                                        \n",
      "Batch # 14 loss=0.4232                                        \n",
      "Batch # 15 loss=0.3976                                        \n",
      "Batch # 16 loss=0.4037                                        \n",
      "Batch # 17 loss=0.4016                                        \n",
      "Batch # 18 loss=0.4278                                        \n",
      "Batch # 19 loss=0.4044                                        \n",
      "Batch # 20 loss=0.3940                                        \n",
      "Batch # 21 loss=0.4250                                        \n",
      "Batch # 22 loss=0.3996                                        \n",
      "Batch # 23 loss=0.4381                                        \n",
      "Batch # 24 loss=0.3916                                        \n",
      "Batch # 25 loss=0.4179                                        \n",
      "Batch # 26 loss=0.3874                                        \n",
      "Batch # 27 loss=0.4210                                        \n",
      "========== EPOCH #13 ========== (0.4198/0.3994)       \n",
      "Batch # 1 loss=0.4084                                        \n",
      "Batch # 2 loss=0.3984                                        \n",
      "Batch # 3 loss=0.3915                                        \n",
      "Batch # 4 loss=0.3818                                        \n",
      "Batch # 5 loss=0.3994                                        \n",
      "Batch # 6 loss=0.3879                                        \n",
      "Batch # 7 loss=0.4023                                        \n",
      "Batch # 8 loss=0.3810                                        \n",
      "Batch # 9 loss=0.4020                                        \n",
      "Batch # 10 loss=0.3814                                        \n",
      "Batch # 11 loss=0.3885                                        \n",
      "Batch # 12 loss=0.4035                                        \n",
      "Batch # 13 loss=0.3848                                        \n",
      "Batch # 14 loss=0.3912                                        \n",
      "Batch # 15 loss=0.3851                                        \n",
      "Batch # 16 loss=0.3742                                        \n",
      "Batch # 17 loss=0.3907                                        \n",
      "Batch # 18 loss=0.3962                                        \n",
      "Batch # 19 loss=0.3763                                        \n",
      "Batch # 20 loss=0.3714                                        \n",
      "Batch # 21 loss=0.3865                                        \n",
      "Batch # 22 loss=0.3942                                        \n",
      "Batch # 23 loss=0.4037                                        \n",
      "Batch # 24 loss=0.3840                                        \n",
      "Batch # 25 loss=0.3852                                        \n",
      "Batch # 26 loss=0.3789                                        \n",
      "Batch # 27 loss=0.4001                                        \n",
      "========== EPOCH #14 ========== (0.3899/0.3843)       \n",
      "Batch # 1 loss=0.3936                                        \n",
      "Batch # 2 loss=0.3880                                        \n",
      "Batch # 3 loss=0.3759                                        \n",
      "Batch # 4 loss=0.3760                                        \n",
      "Batch # 5 loss=0.3825                                        \n",
      "Batch # 6 loss=0.3797                                        \n",
      "Batch # 7 loss=0.3865                                        \n",
      "Batch # 8 loss=0.3821                                        \n",
      "Batch # 9 loss=0.4458                                        \n",
      "Batch # 10 loss=0.4072                                        \n",
      "Batch # 11 loss=0.4352                                        \n",
      "Batch # 12 loss=0.3941                                        \n",
      "Batch # 13 loss=0.4951                                        \n",
      "Batch # 14 loss=0.4684                                        \n",
      "Batch # 15 loss=0.4564                                        \n",
      "Batch # 16 loss=0.3843                                        \n",
      "Batch # 17 loss=0.4709                                        \n",
      "Batch # 18 loss=0.4414                                        \n",
      "Batch # 19 loss=0.3966                                        \n",
      "Batch # 20 loss=0.4355                                        \n",
      "Batch # 21 loss=0.4244                                        \n",
      "Batch # 22 loss=0.4169                                        \n",
      "Batch # 23 loss=0.4575                                        \n",
      "Batch # 24 loss=0.4085                                        \n",
      "Batch # 25 loss=0.4086                                        \n",
      "Batch # 26 loss=0.4151                                        \n",
      "Batch # 27 loss=0.4120                                        \n",
      "========== EPOCH #15 ========== (0.4162/0.4251)       \n",
      "Batch # 1 loss=0.4311                                        \n",
      "Batch # 2 loss=0.4023                                        \n",
      "Batch # 3 loss=0.3973                                        \n",
      "Batch # 4 loss=0.4054                                        \n",
      "Batch # 5 loss=0.3886                                        \n",
      "Batch # 6 loss=0.4156                                        \n",
      "Batch # 7 loss=0.3909                                        \n",
      "Batch # 8 loss=0.3921                                        \n",
      "Batch # 9 loss=0.4121                                        \n",
      "Batch # 10 loss=0.3912                                        \n",
      "Batch # 11 loss=0.3922                                        \n",
      "Batch # 12 loss=0.4048                                        \n",
      "Batch # 13 loss=0.3863                                        \n",
      "Batch # 14 loss=0.3893                                        \n",
      "Batch # 15 loss=0.3905                                        \n",
      "Batch # 16 loss=0.3727                                        \n",
      "Batch # 17 loss=0.4019                                        \n",
      "Batch # 18 loss=0.3962                                        \n",
      "Batch # 19 loss=0.3943                                        \n",
      "Batch # 20 loss=0.3716                                        \n",
      "Batch # 21 loss=0.4049                                        \n",
      "Batch # 22 loss=0.3953                                        \n",
      "Batch # 23 loss=0.4192                                        \n",
      "Batch # 24 loss=0.3845                                        \n",
      "Batch # 25 loss=0.4009                                        \n",
      "Batch # 26 loss=0.3817                                        \n",
      "Batch # 27 loss=0.4067                                        \n",
      "========== EPOCH #16 ========== (0.3970/0.3914)       \n",
      "Batch # 1 loss=0.4002                                        \n",
      "Batch # 2 loss=0.3918                                        \n",
      "Batch # 3 loss=0.3817                                        \n",
      "Batch # 4 loss=0.3787                                        \n",
      "Batch # 5 loss=0.3886                                        \n",
      "Batch # 6 loss=0.3850                                        \n",
      "Batch # 7 loss=0.3918                                        \n",
      "Batch # 8 loss=0.3809                                        \n",
      "Batch # 9 loss=0.3956                                        \n",
      "Batch # 10 loss=0.3717                                        \n",
      "Batch # 11 loss=0.3808                                        \n",
      "Batch # 12 loss=0.3918                                        \n",
      "Batch # 13 loss=0.3772                                        \n",
      "Batch # 14 loss=0.3895                                        \n",
      "Batch # 15 loss=0.3776                                        \n",
      "Batch # 16 loss=0.3739                                        \n",
      "Batch # 17 loss=0.3929                                        \n",
      "Batch # 18 loss=0.3922                                        \n",
      "Batch # 19 loss=0.3874                                        \n",
      "Batch # 20 loss=0.3849                                        \n",
      "Batch # 21 loss=0.3911                                        \n",
      "Batch # 22 loss=0.4346                                        \n",
      "Batch # 23 loss=0.4319                                        \n",
      "Batch # 24 loss=0.3919                                        \n",
      "Batch # 25 loss=0.4172                                        \n",
      "Batch # 26 loss=0.3792                                        \n",
      "Batch # 27 loss=0.4262                                        \n",
      "========== EPOCH #17 ========== (0.3921/0.3862)       \n",
      "Batch # 1 loss=0.3982                                        \n",
      "Batch # 2 loss=0.4035                                        \n",
      "Batch # 3 loss=0.3845                                        \n",
      "Batch # 4 loss=0.3861                                        \n",
      "Batch # 5 loss=0.4034                                        \n",
      "Batch # 6 loss=0.3883                                        \n",
      "Batch # 7 loss=0.4044                                        \n",
      "Batch # 8 loss=0.3859                                        \n",
      "Batch # 9 loss=0.4107                                        \n",
      "Batch # 10 loss=0.3810                                        \n",
      "Batch # 11 loss=0.3837                                        \n",
      "Batch # 12 loss=0.3998                                        \n",
      "Batch # 13 loss=0.3810                                        \n",
      "Batch # 14 loss=0.3942                                        \n",
      "Batch # 15 loss=0.3822                                        \n",
      "Batch # 16 loss=0.3769                                        \n",
      "Batch # 17 loss=0.3876                                        \n",
      "Batch # 18 loss=0.3968                                        \n",
      "Batch # 19 loss=0.3741                                        \n",
      "Batch # 20 loss=0.3701                                        \n",
      "Batch # 21 loss=0.3892                                        \n",
      "Batch # 22 loss=0.3914                                        \n",
      "Batch # 23 loss=0.4049                                        \n",
      "Batch # 24 loss=0.3840                                        \n",
      "Batch # 25 loss=0.3848                                        \n",
      "Batch # 26 loss=0.3850                                        \n",
      "Batch # 27 loss=0.4134                                        \n",
      "========== EPOCH #18 ========== (0.3906/0.3796)       \n",
      "Batch # 1 loss=0.3910                                        \n",
      "Batch # 2 loss=0.4072                                        \n",
      "Batch # 3 loss=0.4101                                        \n",
      "Batch # 4 loss=0.3818                                        \n",
      "Batch # 5 loss=0.4422                                        \n",
      "Batch # 6 loss=0.4077                                        \n",
      "Batch # 7 loss=0.4186                                        \n",
      "Batch # 8 loss=0.3927                                        \n",
      "Batch # 9 loss=0.4112                                        \n",
      "Batch # 10 loss=0.3756                                        \n",
      "Batch # 11 loss=0.4031                                        \n",
      "Batch # 12 loss=0.4028                                        \n",
      "Batch # 13 loss=0.3970                                        \n",
      "Batch # 14 loss=0.4035                                        \n",
      "Batch # 15 loss=0.3864                                        \n",
      "Batch # 16 loss=0.3860                                        \n",
      "Batch # 17 loss=0.3980                                        \n",
      "Batch # 18 loss=0.4015                                        \n",
      "Batch # 19 loss=0.3960                                        \n",
      "Batch # 20 loss=0.3731                                        \n",
      "Batch # 21 loss=0.4062                                        \n",
      "Batch # 22 loss=0.3987                                        \n",
      "Batch # 23 loss=0.4150                                        \n",
      "Batch # 24 loss=0.3922                                        \n",
      "Batch # 25 loss=0.3966                                        \n",
      "Batch # 26 loss=0.3899                                        \n",
      "Batch # 27 loss=0.4007                                        \n",
      "========== EPOCH #19 ========== (0.3994/0.4057)       \n",
      "Batch # 1 loss=0.4135                                        \n",
      "Batch # 2 loss=0.3867                                        \n",
      "Batch # 3 loss=0.3897                                        \n",
      "Batch # 4 loss=0.3760                                        \n",
      "Batch # 5 loss=0.3970                                        \n",
      "Batch # 6 loss=0.3817                                        \n",
      "Batch # 7 loss=0.3982                                        \n",
      "Batch # 8 loss=0.3742                                        \n",
      "Batch # 9 loss=0.3983                                        \n",
      "Batch # 10 loss=0.3715                                        \n",
      "Batch # 11 loss=0.3867                                        \n",
      "Batch # 12 loss=0.3924                                        \n",
      "Batch # 13 loss=0.3888                                        \n",
      "Batch # 14 loss=0.3869                                        \n",
      "Batch # 15 loss=0.3839                                        \n",
      "Batch # 16 loss=0.3709                                        \n",
      "Batch # 17 loss=0.3863                                        \n",
      "Batch # 18 loss=0.3940                                        \n",
      "Batch # 19 loss=0.3725                                        \n",
      "Batch # 20 loss=0.3683                                        \n",
      "Batch # 21 loss=0.3859                                        \n",
      "Batch # 22 loss=0.3912                                        \n",
      "Batch # 23 loss=0.4007                                        \n",
      "Batch # 24 loss=0.3803                                        \n",
      "Batch # 25 loss=0.3845                                        \n",
      "Batch # 26 loss=0.3789                                        \n",
      "Batch # 27 loss=0.4012                                        \n",
      "========== EPOCH #20 ========== (0.3867/0.3791)       \n",
      "Batch # 1 loss=0.3898                                        \n",
      "Batch # 2 loss=0.3843                                        \n",
      "Batch # 3 loss=0.3739                                        \n",
      "Batch # 4 loss=0.3746                                        \n",
      "Batch # 5 loss=0.3846                                        \n",
      "Batch # 6 loss=0.3817                                        \n",
      "Batch # 7 loss=0.3850                                        \n",
      "Batch # 8 loss=0.3779                                        \n",
      "Batch # 9 loss=0.4125                                        \n",
      "Batch # 10 loss=0.3768                                        \n",
      "Batch # 11 loss=0.3800                                        \n",
      "Batch # 12 loss=0.3938                                        \n",
      "Batch # 13 loss=0.3851                                        \n",
      "Batch # 14 loss=0.3936                                        \n",
      "Batch # 15 loss=0.3772                                        \n",
      "Batch # 16 loss=0.3817                                        \n",
      "Batch # 17 loss=0.3991                                        \n",
      "Batch # 18 loss=0.3961                                        \n",
      "Batch # 19 loss=0.4009                                        \n",
      "Batch # 20 loss=0.3747                                        \n",
      "Batch # 21 loss=0.4066                                        \n",
      "Batch # 22 loss=0.3954                                        \n",
      "Batch # 23 loss=0.4169                                        \n",
      "Batch # 24 loss=0.3874                                        \n",
      "Batch # 25 loss=0.4013                                        \n",
      "Batch # 26 loss=0.3810                                        \n",
      "Batch # 27 loss=0.4084                                        \n",
      "========== EPOCH #21 ========== (0.3896/0.3880)       \n",
      "Batch # 1 loss=0.3970                                        \n",
      "Batch # 2 loss=0.3894                                        \n",
      "Batch # 3 loss=0.3817                                        \n",
      "Batch # 4 loss=0.3775                                        \n",
      "Batch # 5 loss=0.3903                                        \n",
      "Batch # 6 loss=0.3824                                        \n",
      "Batch # 7 loss=0.3933                                        \n",
      "Batch # 8 loss=0.3755                                        \n",
      "Batch # 9 loss=0.3955                                        \n",
      "Batch # 10 loss=0.3714                                        \n",
      "Batch # 11 loss=0.3833                                        \n",
      "Batch # 12 loss=0.3911                                        \n",
      "Batch # 13 loss=0.3850                                        \n",
      "Batch # 14 loss=0.3900                                        \n",
      "Batch # 15 loss=0.3789                                        \n",
      "Batch # 16 loss=0.3795                                        \n",
      "Batch # 17 loss=0.3935                                        \n",
      "Batch # 18 loss=0.3928                                        \n",
      "Batch # 19 loss=0.3949                                        \n",
      "Batch # 20 loss=0.3881                                        \n",
      "Batch # 21 loss=0.3971                                        \n",
      "Batch # 22 loss=0.4319                                        \n",
      "Batch # 23 loss=0.4077                                        \n",
      "Batch # 24 loss=0.3921                                        \n",
      "Batch # 25 loss=0.3894                                        \n",
      "Batch # 26 loss=0.3872                                        \n",
      "Batch # 27 loss=0.4077                                        \n",
      "========== EPOCH #22 ========== (0.3905/0.3892)       \n",
      "Batch # 1 loss=0.4023                                        \n",
      "Batch # 2 loss=0.3877                                        \n",
      "Batch # 3 loss=0.3860                                        \n",
      "Batch # 4 loss=0.3781                                        \n",
      "Batch # 5 loss=0.3961                                        \n",
      "Batch # 6 loss=0.3847                                        \n",
      "Batch # 7 loss=0.3957                                        \n",
      "Batch # 8 loss=0.3813                                        \n",
      "Batch # 9 loss=0.4059                                        \n",
      "Batch # 10 loss=0.3738                                        \n",
      "Batch # 11 loss=0.3826                                        \n",
      "Batch # 12 loss=0.3935                                        \n",
      "Batch # 13 loss=0.3756                                        \n",
      "Batch # 14 loss=0.3900                                        \n",
      "Batch # 15 loss=0.3783                                        \n",
      "Batch # 16 loss=0.3732                                        \n",
      "Batch # 17 loss=0.3906                                        \n",
      "Batch # 18 loss=0.3934                                        \n",
      "Batch # 19 loss=0.3860                                        \n",
      "Batch # 20 loss=0.3709                                        \n",
      "Batch # 21 loss=0.3943                                        \n",
      "Batch # 22 loss=0.4033                                        \n",
      "Batch # 23 loss=0.3995                                        \n",
      "Batch # 24 loss=0.3837                                        \n",
      "Batch # 25 loss=0.3857                                        \n",
      "Batch # 26 loss=0.3777                                        \n",
      "Batch # 27 loss=0.4065                                        \n",
      "========== EPOCH #23 ========== (0.3880/0.3793)       \n",
      "Batch # 1 loss=0.3901                                        \n",
      "Batch # 2 loss=0.3904                                        \n",
      "Batch # 3 loss=0.3759                                        \n",
      "Batch # 4 loss=0.3798                                        \n",
      "Batch # 5 loss=0.3889                                        \n",
      "Batch # 6 loss=0.3793                                        \n",
      "Batch # 7 loss=0.3918                                        \n",
      "Batch # 8 loss=0.3770                                        \n",
      "Batch # 9 loss=0.3958                                        \n",
      "Batch # 10 loss=0.3701                                        \n",
      "Batch # 11 loss=0.3799                                        \n",
      "Batch # 12 loss=0.3906                                        \n",
      "Batch # 13 loss=0.3797                                        \n",
      "Batch # 14 loss=0.3881                                        \n",
      "Batch # 15 loss=0.3766                                        \n",
      "Batch # 16 loss=0.3752                                        \n",
      "Batch # 17 loss=0.3918                                        \n",
      "Batch # 18 loss=0.3911                                        \n",
      "Batch # 19 loss=0.3858                                        \n",
      "Batch # 20 loss=0.3773                                        \n",
      "Batch # 21 loss=0.3942                                        \n",
      "Batch # 22 loss=0.4106                                        \n",
      "Batch # 23 loss=0.4017                                        \n",
      "Batch # 24 loss=0.3863                                        \n",
      "Batch # 25 loss=0.3869                                        \n",
      "Batch # 26 loss=0.3789                                        \n",
      "Batch # 27 loss=0.4062                                        \n",
      "========== EPOCH #24 ========== (0.3867/0.3816)       \n",
      "Batch # 1 loss=0.3927                                        \n",
      "Batch # 2 loss=0.3917                                        \n",
      "Batch # 3 loss=0.3731                                        \n",
      "Batch # 4 loss=0.3814                                        \n",
      "Batch # 5 loss=0.3827                                        \n",
      "Batch # 6 loss=0.3884                                        \n",
      "Batch # 7 loss=0.3869                                        \n",
      "Batch # 8 loss=0.3746                                        \n",
      "Batch # 9 loss=0.3944                                        \n",
      "Batch # 10 loss=0.3792                                        \n",
      "Batch # 11 loss=0.3828                                        \n",
      "Batch # 12 loss=0.3967                                        \n",
      "Batch # 13 loss=0.3884                                        \n",
      "Batch # 14 loss=0.3849                                        \n",
      "Batch # 15 loss=0.3824                                        \n",
      "Batch # 16 loss=0.3706                                        \n",
      "Batch # 17 loss=0.3845                                        \n",
      "Batch # 18 loss=0.3927                                        \n",
      "Batch # 19 loss=0.3726                                        \n",
      "Batch # 20 loss=0.3674                                        \n",
      "Batch # 21 loss=0.3865                                        \n",
      "Batch # 22 loss=0.3908                                        \n",
      "Batch # 23 loss=0.3994                                        \n",
      "Batch # 24 loss=0.3805                                        \n",
      "Batch # 25 loss=0.3833                                        \n",
      "Batch # 26 loss=0.3785                                        \n",
      "Batch # 27 loss=0.4038                                        \n",
      "========== EPOCH #25 ========== (0.3849/0.3786)       \n",
      "Batch # 1 loss=0.3894                                        \n",
      "Batch # 2 loss=0.3867                                        \n",
      "Batch # 3 loss=0.3774                                        \n",
      "Batch # 4 loss=0.3753                                        \n",
      "Batch # 5 loss=0.3899                                        \n",
      "Batch # 6 loss=0.3883                                        \n",
      "Batch # 7 loss=0.3863                                        \n",
      "Batch # 8 loss=0.3921                                        \n",
      "Batch # 9 loss=0.4250                                        \n",
      "Batch # 10 loss=0.3698                                        \n",
      "Batch # 11 loss=0.4020                                        \n",
      "Batch # 12 loss=0.3998                                        \n",
      "Batch # 13 loss=0.3812                                        \n",
      "Batch # 14 loss=0.3885                                        \n",
      "Batch # 15 loss=0.3842                                        \n",
      "Batch # 16 loss=0.3730                                        \n",
      "Batch # 17 loss=0.3972                                        \n",
      "Batch # 18 loss=0.3937                                        \n",
      "Batch # 19 loss=0.3892                                        \n",
      "Batch # 20 loss=0.3707                                        \n",
      "Batch # 21 loss=0.4030                                        \n",
      "Batch # 22 loss=0.3931                                        \n",
      "Batch # 23 loss=0.4165                                        \n",
      "Batch # 24 loss=0.3834                                        \n",
      "Batch # 25 loss=0.3985                                        \n",
      "Batch # 26 loss=0.3805                                        \n",
      "Batch # 27 loss=0.4041                                        \n",
      "========== EPOCH #26 ========== (0.3903/0.3918)       \n",
      "Batch # 1 loss=0.4001                                        \n",
      "Batch # 2 loss=0.3889                                        \n",
      "Batch # 3 loss=0.3808                                        \n",
      "Batch # 4 loss=0.3775                                        \n",
      "Batch # 5 loss=0.3894                                        \n",
      "Batch # 6 loss=0.3828                                        \n",
      "Batch # 7 loss=0.3920                                        \n",
      "Batch # 8 loss=0.3823                                        \n",
      "Batch # 9 loss=0.3947                                        \n",
      "Batch # 10 loss=0.3697                                        \n",
      "Batch # 11 loss=0.3792                                        \n",
      "Batch # 12 loss=0.3903                                        \n",
      "Batch # 13 loss=0.3748                                        \n",
      "Batch # 14 loss=0.3873                                        \n",
      "Batch # 15 loss=0.3768                                        \n",
      "Batch # 16 loss=0.3705                                        \n",
      "Batch # 17 loss=0.3918                                        \n",
      "Batch # 18 loss=0.3914                                        \n",
      "Batch # 19 loss=0.3778                                        \n",
      "Batch # 20 loss=0.3796                                        \n",
      "Batch # 21 loss=0.3850                                        \n",
      "Batch # 22 loss=0.4184                                        \n",
      "Batch # 23 loss=0.4308                                        \n",
      "Batch # 24 loss=0.3821                                        \n",
      "Batch # 25 loss=0.4187                                        \n",
      "Batch # 26 loss=0.3840                                        \n",
      "Batch # 27 loss=0.4255                                        \n",
      "========== EPOCH #27 ========== (0.3897/0.3811)       \n",
      "Batch # 1 loss=0.3914                                        \n",
      "Batch # 2 loss=0.4084                                        \n",
      "Batch # 3 loss=0.3750                                        \n",
      "Batch # 4 loss=0.3940                                        \n",
      "Batch # 5 loss=0.3942                                        \n",
      "Batch # 6 loss=0.3921                                        \n",
      "Batch # 7 loss=0.4028                                        \n",
      "Batch # 8 loss=0.3781                                        \n",
      "Batch # 9 loss=0.4171                                        \n",
      "Batch # 10 loss=0.3736                                        \n",
      "Batch # 11 loss=0.3892                                        \n",
      "Batch # 12 loss=0.3938                                        \n",
      "Batch # 13 loss=0.3789                                        \n",
      "Batch # 14 loss=0.3908                                        \n",
      "Batch # 15 loss=0.3802                                        \n",
      "Batch # 16 loss=0.3744                                        \n",
      "Batch # 17 loss=0.3932                                        \n",
      "Batch # 18 loss=0.3938                                        \n",
      "Batch # 19 loss=0.3892                                        \n",
      "Batch # 20 loss=0.3690                                        \n",
      "Batch # 21 loss=0.3990                                        \n",
      "Batch # 22 loss=0.3963                                        \n",
      "Batch # 23 loss=0.4025                                        \n",
      "Batch # 24 loss=0.3884                                        \n",
      "Batch # 25 loss=0.3841                                        \n",
      "Batch # 26 loss=0.3886                                        \n",
      "Batch # 27 loss=0.4050                                        \n",
      "========== EPOCH #28 ========== (0.3905/0.3830)       \n",
      "Batch # 1 loss=0.3954                                        \n",
      "Batch # 2 loss=0.3933                                        \n",
      "Batch # 3 loss=0.3719                                        \n",
      "Batch # 4 loss=0.3831                                        \n",
      "Batch # 5 loss=0.3819                                        \n",
      "Batch # 6 loss=0.3896                                        \n",
      "Batch # 7 loss=0.3867                                        \n",
      "Batch # 8 loss=0.3759                                        \n",
      "Batch # 9 loss=0.3943                                        \n",
      "Batch # 10 loss=0.3803                                        \n",
      "Batch # 11 loss=0.3818                                        \n",
      "Batch # 12 loss=0.3991                                        \n",
      "Batch # 13 loss=0.3815                                        \n",
      "Batch # 14 loss=0.3862                                        \n",
      "Batch # 15 loss=0.3817                                        \n",
      "Batch # 16 loss=0.3689                                        \n",
      "Batch # 17 loss=0.3860                                        \n",
      "Batch # 18 loss=0.3915                                        \n",
      "Batch # 19 loss=0.3718                                        \n",
      "Batch # 20 loss=0.3669                                        \n",
      "Batch # 21 loss=0.3829                                        \n",
      "Batch # 22 loss=0.3902                                        \n",
      "Batch # 23 loss=0.3986                                        \n",
      "Batch # 24 loss=0.3792                                        \n",
      "Batch # 25 loss=0.3835                                        \n",
      "Batch # 26 loss=0.3760                                        \n",
      "Batch # 27 loss=0.3985                                        \n",
      "========== EPOCH #29 ========== (0.3843/0.3807)       \n",
      "Batch # 1 loss=0.3906                                        \n",
      "Batch # 2 loss=0.3821                                        \n",
      "Batch # 3 loss=0.3702                                        \n",
      "Batch # 4 loss=0.3740                                        \n",
      "Batch # 5 loss=0.3806                                        \n",
      "Batch # 6 loss=0.3783                                        \n",
      "Batch # 7 loss=0.3846                                        \n",
      "Batch # 8 loss=0.3721                                        \n",
      "Batch # 9 loss=0.3959                                        \n",
      "Batch # 10 loss=0.3809                                        \n",
      "Batch # 11 loss=0.3980                                        \n",
      "Batch # 12 loss=0.3889                                        \n",
      "Batch # 13 loss=0.4063                                        \n",
      "Batch # 14 loss=0.4423                                        \n",
      "Batch # 15 loss=0.3891                                        \n",
      "Batch # 16 loss=0.4337                                        \n",
      "Batch # 17 loss=0.3877                                        \n",
      "Batch # 18 loss=0.4246                                        \n",
      "Batch # 19 loss=0.3804                                        \n",
      "Batch # 20 loss=0.3950                                        \n",
      "Batch # 21 loss=0.3983                                        \n",
      "Batch # 22 loss=0.4063                                        \n",
      "Batch # 23 loss=0.4202                                        \n",
      "Batch # 24 loss=0.3906                                        \n",
      "Batch # 25 loss=0.4022                                        \n",
      "Batch # 26 loss=0.3834                                        \n",
      "Batch # 27 loss=0.4158                                        \n",
      "========== EPOCH #30 ========== (0.3953/0.3885)       \n"
     ]
    }
   ],
   "source": [
    "# restart = True\n",
    "# num_test = 0\n",
    "# while restart:\n",
    "#     num_test += 1\n",
    "#     print(\"=/\"*10 + \"NUMTEST \" + str(num_test) + \" \" + \"=/\"*30)\n",
    "models = get_models(params)\n",
    "restart = train_model(params, models, dataset, f\"{num_test}_deep_torch_11000\", save_frequency=5, restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2674a-fb87-4c50-a726-32d39a29bbcd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_weigths(models, path_node_gnn, path_edge_linear):\n",
    "    models[0].load_state_dict(torch.load(path_node_gnn, weights_only=True))\n",
    "    models[1].load_state_dict(torch.load(path_edge_linear, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca54f343-cb1a-4731-9c2c-c0d71d48c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_load = get_models(params)\n",
    "load_weigths(models_load, \"deep_torch_11000_node_gnn_end\", \"deep_torch_11000_edge_linear_end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d097ece-7450-4682-b01d-f178dc020a42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def torch_classification_edges(models, graph, k=0.51):\n",
    "    i = graph[\"A\"]\n",
    "    v_in = [rev_dist(e) for e in graph[\"edges_feature\"]]\n",
    "    x = graph[\"nodes_feature\"]\n",
    "    N = len(x)\n",
    "    X = torch.tensor(data=x, dtype=torch.float32)\n",
    "    sp_A = torch.sparse_coo_tensor(indices=i, values=v_in, size=(N, N), dtype=torch.float32)\n",
    "    \n",
    "    H_end = models[0](X, sp_A)\n",
    "    Omega = torch.cat([H_end[i[0]], H_end[i[1]]],dim=1)\n",
    "    E_pred = models[1](Omega)\n",
    "    a = np.zeros(E_pred.shape)\n",
    "    return E_pred\n",
    "    a[E_pred>k] = 1.0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecfdfcd6-00c4-4813-bb17-29d5a86bd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch_classification_edges(models, dataset[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bac399b2-9eee-4986-992a-6fc2b542b441",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mask == np.array(dataset[2]['true_edges']))/ len(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ca165-54bb-435f-87e6-436d5935df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.90 -  1500 \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7192cb90-9f67-4d5f-816a-32883794b407",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in dataset[2]['true_edges']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2050b-e55f-4ddc-8a75-7b94a53d43c8",
   "metadata": {},
   "source": [
    "# asGLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ffb5752-9a07-4460-aea9-246ecc024c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import BatchNorm, TAGConv\n",
    "from torch.nn import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d21ea08-b3e1-4673-b181-669bd703b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeGLAM(torch.nn.Module):\n",
    "    def __init__(self,  input_, output_):\n",
    "        super(NodeGLAM, self).__init__()\n",
    "        self.batch_norm1 = BatchNorm(input_)\n",
    "        \n",
    "        h1 = 16                                 #<------------------------\n",
    "        self.linear1 = Linear(input_, h1) \n",
    "        h2 = 16                                 #<------------------------\n",
    "        self.tag1 = TAGConv(h1, h2)\n",
    "        \n",
    "        h3 = 16                                 #<------------------------\n",
    "        self.linear2 = Linear(h2, h3) \n",
    "        h4 = 16                                 #<------------------------\n",
    "        self.tag2 = TAGConv(h3, h4)\n",
    "\n",
    "        h5 = 16                                 #<------------------------\n",
    "        self.linear3 = Linear(h4+input_, h5)\n",
    "        h6 = 16                                 #<------------------------\n",
    "        self.linear4 =Linear(h5, output_)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.batch_norm1(x)\n",
    "        h = self.linear1(x)\n",
    "        h = relu(h)\n",
    "        h = self.tag1(h, edge_index)\n",
    "        h = relu(h)\n",
    "        \n",
    "        h = self.linear2(h)\n",
    "        h = relu(h)\n",
    "        h = self.tag2(h, edge_index)\n",
    "        h = relu(h)\n",
    "        a = torch.cat([x, h], dim=1)\n",
    "        a = self.linear3(a)\n",
    "        a = relu(a)\n",
    "        a = self.linear4(a)\n",
    "        return torch.softmax(a, dim=-1)\n",
    "\n",
    "class EdgeGLAM(torch.nn.Module):\n",
    "    def __init__(self, input_, output_):\n",
    "        super(EdgeGLAM, self).__init__()\n",
    "        self.batch_norm2 = BatchNorm(input_, output_)\n",
    "        h1 = 16                                 #<------------------------\n",
    "        self.linear1 = Linear(input_, h1) \n",
    "        self.linear2 = Linear(h1, output_)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.batch_norm2(x)\n",
    "        h = self.linear1(x)\n",
    "        h = relu(h)\n",
    "        h = self.linear2(h)\n",
    "        h = torch.sigmoid(h)\n",
    "        return torch.squeeze(h, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f618a9b-96d8-40a8-811d-2525666e3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_glam = NodeGLAM(9, 5)\n",
    "edge_glam = EdgeGLAM(2*9+2*5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8164e706-e833-4f9a-8518-31a769377eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_batchs(dataset, batch_size):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i:i+batch_size]\n",
    "\n",
    "def get_tensor_from_graph(graph):\n",
    "    i = graph[\"A\"]\n",
    "    v_in = [rev_dist(e) for e in graph[\"edges_feature\"]]\n",
    "    v_true = graph[\"true_edges\"]\n",
    "    x = graph[\"nodes_feature\"]\n",
    "    N = len(x)\n",
    "    \n",
    "    X = torch.tensor(data=x, dtype=torch.float32)\n",
    "    sp_A = torch.sparse_coo_tensor(indices=i, values=v_in, size=(N, N), dtype=torch.float32)\n",
    "    E_true = torch.tensor(data=v_true, dtype=torch.float32)\n",
    "    return X, sp_A, E_true, i\n",
    "\n",
    "def validation(models, dataset, criterion):\n",
    "    my_loss_list = []\n",
    "    for j, graph in enumerate(dataset):\n",
    "        X, sp_A, E_true, i = get_tensor_from_graph(graph)\n",
    "        Node_emb = models[0](X, sp_A)\n",
    "        Omega = torch.cat([Node_emb[i[0]],Node_emb[i[1]], X[i[0]], X[i[1]]],dim=1)\n",
    "        E_pred = models[1](Omega)\n",
    "        loss = criterion(E_pred, E_true)\n",
    "        my_loss_list.append(loss.item())\n",
    "        print(f\"{(j+1)/len(dataset)*100:.2f} % loss = {my_loss_list[-1]:.5f} {' '*30}\", end='\\r')\n",
    "    return np.mean(my_loss_list)\n",
    "\n",
    "def split_train_val(dataset, val_split=0.2, shuffle=True, seed=1234):\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(dataset)\n",
    "    train_size = int(len(dataset) * (1 - val_split))\n",
    "    train_dataset = dataset[:train_size]\n",
    "    val_dataset = dataset[train_size:]\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def train_step(models, batch, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    my_loss_list = []\n",
    "   \n",
    "    for j, graph in enumerate(batch):\n",
    "        X, sp_A, E_true, i = get_tensor_from_graph(dataset[0])\n",
    "        Node_emb = models[0](X, sp_A)\n",
    "        Omega = torch.cat([Node_emb[i[0]],Node_emb[i[1]], X[i[0]], X[i[1]]],dim=1)\n",
    "        E_pred = models[1](Omega)\n",
    "        loss = criterion(E_pred, E_true)\n",
    "        my_loss_list.append(loss.item())\n",
    "        print(f\"Batch loss={my_loss_list[-1]:.4f}\" + \" \"*40, end=\"\\r\")\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    return np.mean(my_loss_list)\n",
    "\n",
    "def train_model(params, models, dataset, path_save, save_frequency=5, restart=False):  \n",
    "    optimizer = torch.optim.Adam(\n",
    "    list(models[0].parameters()) + list(models[1].parameters()),\n",
    "    lr=params[\"learning_rate\"],\n",
    "    )\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss_list = []\n",
    "    with open('log.txt', 'a') as f:\n",
    "        for key, val in params.items():\n",
    "            f.write(f\"{key}:\\t{val}\\n\")\n",
    "    train_dataset, val_dataset = split_train_val(dataset, val_split=0.1)\n",
    "    for k in range(params[\"epochs\"]):\n",
    "        my_loss_list = []\n",
    "        \n",
    "        for l, batch in enumerate(list_batchs(train_dataset, params[\"batch_size\"])):\n",
    "            batch_loss = train_step(models, batch, optimizer, criterion)\n",
    "            my_loss_list.append(batch_loss)\n",
    "            print(f\"Batch # {l+1} loss={my_loss_list[-1]:.4f}\" + \" \"*40)\n",
    "        train_val = np.mean(my_loss_list)\n",
    "        loss_list.append(train_val)\n",
    "        validation_val = validation(models, val_dataset, criterion)\n",
    "        print(\"=\"*10, f\"EPOCH #{k+1}\",\"=\"*10, f\"({train_val:.4f}/{validation_val:.4f})\")\n",
    "        \n",
    "        # TODO: DELETE RESTART\n",
    "        if restart and k>=2 and abs(loss_list[k] - loss_list[k-1]) < 0.001:\n",
    "            return True\n",
    "            \n",
    "            \n",
    "        with open('log.txt', 'a') as f:\n",
    "            f.write(f\"EPOCH #{k}\\t {train_val:.8f} (VAL: {validation_val:.8f})\\n\")  \n",
    "        if (k+1) % save_frequency == 0:\n",
    "            num = k//save_frequency\n",
    "            torch.save(models[0].state_dict(), path_save+f\"_node_gnn_{num}\")\n",
    "            torch.save(models[1].state_dict(), path_save+f\"_edge_linear_{num}\")\n",
    "    torch.save(models[0].state_dict(), path_save+f\"_node_gnn_end\")\n",
    "    torch.save(models[1].state_dict(), path_save+f\"_edge_linear_end\")\n",
    "    return False # For restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4ed5203-789b-4f31-8f8e-774af2e9485f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/project/PageR/env/lib/python3.12/site-packages/torch_geometric/utils/_spmm.py:66: UserWarning: Converting sparse tensor to CSR format for more efficient processing. Consider converting your sparse tensor to CSR format beforehand to avoid repeated conversion (got 'torch.sparse_coo')\n",
      "  warnings.warn(f\"Converting sparse tensor to CSR format for more \"\n",
      "/home/daniil/project/PageR/env/lib/python3.12/site-packages/torch_geometric/utils/_spmm.py:70: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  src = src.to_sparse_csr()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([925]), torch.Size([451, 9]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, sp_A, E_true, i = get_tensor_from_graph(dataset[0])\n",
    "Node_emb = node_glam(X, sp_A)\n",
    "Omega = torch.cat([Node_emb[i[0]],Node_emb[i[1]], X[i[0]], X[i[1]]],dim=1)\n",
    "Edge_emb = edge_glam(Omega)\n",
    "\n",
    "Edge_emb.shape,X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7a4a2ae-ac11-4249-9770-1f392788a7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 1 loss=0.7919                                        \n",
      "Batch # 2 loss=0.6893                                        \n",
      "Batch # 3 loss=0.5681                                        \n",
      "Batch # 4 loss=0.4458                                        \n",
      "Batch # 5 loss=0.3943                                        \n",
      "Batch # 6 loss=0.4412                                        \n",
      "Batch # 7 loss=0.4576                                        \n",
      "Batch # 8 loss=0.4256                                        \n",
      "Batch # 9 loss=0.3860                                        \n",
      "Batch # 10 loss=0.3658                                        \n",
      "Batch # 11 loss=0.3670                                        \n",
      "Batch # 12 loss=0.3707                                        \n",
      "Batch # 13 loss=0.3720                                        \n",
      "Batch # 14 loss=0.3664                                        \n",
      "Batch # 15 loss=0.3569                                        \n",
      "Batch # 16 loss=0.3489                                        \n",
      "Batch # 17 loss=0.3356                                        \n",
      "Batch # 18 loss=0.3456                                        \n",
      "Batch # 19 loss=0.3394                                        \n",
      "Batch # 20 loss=0.3336                                        \n",
      "Batch # 21 loss=0.3238                                        \n",
      "Batch # 22 loss=0.3165                                        \n",
      "Batch # 23 loss=0.3077                                        \n",
      "Batch # 24 loss=0.3085                                        \n",
      "Batch # 25 loss=0.3009                                        \n",
      "Batch # 26 loss=0.3117                                        \n",
      "Batch # 27 loss=0.3069                                        \n",
      "Batch # 28 loss=0.2934                                        \n",
      "Batch # 29 loss=0.2848                                        \n",
      "Batch # 30 loss=0.3088                                        \n",
      "Batch # 31 loss=0.2909                                        \n",
      "Batch # 32 loss=0.2854                                        \n",
      "Batch # 33 loss=0.2874                                        \n",
      "Batch # 34 loss=0.2823                                        \n",
      "Batch # 35 loss=0.2802                                        \n",
      "Batch # 36 loss=0.2808                                        \n",
      "Batch # 37 loss=0.2754                                        \n",
      "Batch # 38 loss=0.2752                                        \n",
      "Batch # 39 loss=0.2761                                        \n",
      "Batch # 40 loss=0.2724                                        \n",
      "Batch # 41 loss=0.2648                                        \n",
      "Batch # 42 loss=0.2636                                        \n",
      "Batch # 43 loss=0.2621                                        \n",
      "Batch # 44 loss=0.2570                                        \n",
      "Batch # 45 loss=0.2586                                        \n",
      "Batch # 46 loss=0.2561                                        \n",
      "Batch # 47 loss=0.2527                                        \n",
      "Batch # 48 loss=0.2501                                        \n",
      "Batch # 49 loss=0.2490                                        \n",
      "Batch # 50 loss=0.2502                                        \n",
      "Batch # 51 loss=0.2396                                        \n",
      "Batch # 52 loss=0.2420                                        \n",
      "Batch # 53 loss=0.2358                                        \n",
      "Batch # 54 loss=0.2343                                        \n",
      "Batch # 55 loss=0.2330                                        \n",
      "Batch # 56 loss=0.2290                                        \n",
      "Batch # 57 loss=0.2266                                        \n",
      "Batch # 58 loss=0.2246                                        \n",
      "Batch # 59 loss=0.2213                                        \n",
      "Batch # 60 loss=0.2190                                        \n",
      "Batch # 61 loss=0.2146                                        \n",
      "Batch # 62 loss=0.2116                                        \n",
      "Batch # 63 loss=0.2116                                        \n",
      "Batch # 64 loss=0.2103                                        \n",
      "Batch # 65 loss=0.2083                                        \n",
      "Batch # 66 loss=0.2087                                        \n",
      "Batch # 67 loss=0.2002                                        \n",
      "Batch # 68 loss=0.1948                                        \n",
      "Batch # 69 loss=0.1930                                        \n",
      "Batch # 70 loss=0.1951                                        \n",
      "Batch # 71 loss=0.1871                                        \n",
      "Batch # 72 loss=0.1892                                        \n",
      "Batch # 73 loss=0.1829                                        \n",
      "Batch # 74 loss=0.1814                                        \n",
      "Batch # 75 loss=0.1823                                        \n",
      "Batch # 76 loss=0.1749                                        \n",
      "Batch # 77 loss=0.1700                                        \n",
      "Batch # 78 loss=0.1677                                        \n",
      "Batch # 79 loss=0.1675                                        \n",
      "Batch # 80 loss=0.1660                                        \n",
      "Batch # 81 loss=0.1652                                        \n",
      "Batch # 82 loss=0.1600                                        \n",
      "Batch # 83 loss=0.1569                                        \n",
      "Batch # 84 loss=0.1540                                        \n",
      "Batch # 85 loss=0.1542                                        \n",
      "Batch # 86 loss=0.1519                                        \n",
      "Batch # 87 loss=0.1631                                        \n",
      "Batch # 88 loss=0.1548                                        \n",
      "Batch # 89 loss=0.1514                                        \n",
      "Batch # 90 loss=0.1431                                        \n",
      "Batch # 91 loss=0.1482                                        \n",
      "Batch # 92 loss=0.1520                                        \n",
      "Batch # 93 loss=0.1365                                        \n",
      "Batch # 94 loss=0.1475                                        \n",
      "Batch # 95 loss=0.1501                                        \n",
      "Batch # 96 loss=0.1363                                        \n",
      "Batch # 97 loss=0.1515                                        \n",
      "Batch # 98 loss=0.1314                                        \n",
      "Batch # 99 loss=0.1391                                        \n",
      "Batch # 100 loss=0.1252                                        \n",
      "Batch # 101 loss=0.1335                                        \n",
      "Batch # 102 loss=0.1460                                        \n",
      "Batch # 103 loss=0.1299                                        \n",
      "Batch # 104 loss=0.1218                                        \n",
      "Batch # 105 loss=0.1203                                        \n",
      "Batch # 106 loss=0.1197                                        \n",
      "Batch # 107 loss=0.1170                                        \n",
      "Batch # 108 loss=0.1213                                        \n",
      "========== EPOCH #1 ========== (0.2504/0.9327)        \n",
      "Batch # 1 loss=0.1520                                        \n",
      "Batch # 2 loss=0.1381                                        \n",
      "Batch # 3 loss=0.1494                                        \n",
      "Batch # 4 loss=0.1129                                        \n",
      "Batch # 5 loss=0.1535                                        \n",
      "Batch # 6 loss=0.1137                                        \n",
      "Batch # 7 loss=0.1368                                        \n",
      "Batch # 8 loss=0.1284                                        \n",
      "Batch # 9 loss=0.1161                                        \n",
      "Batch # 10 loss=0.1217                                        \n",
      "Batch # 11 loss=0.1345                                        \n",
      "Batch # 12 loss=0.1169                                        \n",
      "Batch # 13 loss=0.1233                                        \n",
      "Batch # 14 loss=0.1168                                        \n",
      "Batch # 15 loss=0.1232                                        \n",
      "Batch # 16 loss=0.1174                                        \n",
      "Batch # 17 loss=0.1064                                        \n",
      "Batch # 18 loss=0.1050                                        \n",
      "Batch # 19 loss=0.1175                                        \n",
      "Batch # 20 loss=0.1839                                        \n",
      "Batch # 21 loss=0.1652                                        \n",
      "Batch # 22 loss=0.1623                                        \n",
      "Batch # 23 loss=0.1584                                        \n",
      "Batch # 24 loss=0.1427                                        \n",
      "Batch # 25 loss=0.1241                                        \n",
      "Batch # 26 loss=0.1300                                        \n",
      "Batch # 27 loss=0.1207                                        \n",
      "Batch # 28 loss=0.1365                                        \n",
      "Batch # 29 loss=0.1364                                        \n",
      "Batch # 30 loss=0.1173                                        \n",
      "Batch # 31 loss=0.1612                                        \n",
      "Batch # 32 loss=0.1271                                        \n",
      "Batch # 33 loss=0.1358                                        \n",
      "Batch # 34 loss=0.1378                                        \n",
      "Batch # 35 loss=0.1666                                        \n",
      "Batch # 36 loss=0.1579                                        \n",
      "Batch # 37 loss=0.1359                                        \n",
      "Batch # 38 loss=0.1413                                        \n",
      "Batch # 39 loss=0.1259                                        \n",
      "Batch # 40 loss=0.1297                                        \n",
      "Batch # 41 loss=0.1202                                        \n",
      "Batch # 42 loss=0.1202                                        \n",
      "Batch # 43 loss=0.1179                                        \n",
      "Batch # 44 loss=0.1171                                        \n",
      "Batch # 45 loss=0.1211                                        \n",
      "Batch # 46 loss=0.1178                                        \n",
      "Batch # 47 loss=0.1287                                        \n",
      "Batch # 48 loss=0.1281                                        \n",
      "Batch # 49 loss=0.1243                                        \n",
      "Batch # 50 loss=0.1147                                        \n",
      "Batch # 51 loss=0.1099                                        \n",
      "Batch # 52 loss=0.1066                                        \n",
      "Batch # 53 loss=0.1057                                        \n",
      "Batch # 54 loss=0.0991                                        \n",
      "Batch # 55 loss=0.0961                                        \n",
      "Batch # 56 loss=0.0946                                        \n",
      "Batch # 57 loss=0.0922                                        \n",
      "Batch # 58 loss=0.0911                                        \n",
      "Batch # 59 loss=0.0865                                        \n",
      "Batch # 60 loss=0.0845                                        \n",
      "Batch # 61 loss=0.0832                                        \n",
      "Batch # 62 loss=0.0824                                        \n",
      "Batch # 63 loss=0.0815                                        \n",
      "Batch # 64 loss=0.0801                                        \n",
      "Batch # 65 loss=0.0786                                        \n",
      "Batch # 66 loss=0.0766                                        \n",
      "Batch # 67 loss=0.0758                                        \n",
      "Batch # 68 loss=0.0750                                        \n",
      "Batch # 69 loss=0.0740                                        \n",
      "Batch # 70 loss=0.0727                                        \n",
      "Batch # 71 loss=0.0714                                        \n",
      "Batch # 72 loss=0.0706                                        \n",
      "Batch # 73 loss=0.0692                                        \n",
      "Batch # 74 loss=0.0697                                        \n",
      "Batch # 75 loss=0.0693                                        \n",
      "Batch # 76 loss=0.0684                                        \n",
      "Batch # 77 loss=0.0674                                        \n",
      "Batch # 78 loss=0.0670                                        \n",
      "Batch # 79 loss=0.0659                                        \n",
      "Batch # 80 loss=0.0659                                        \n",
      "Batch # 81 loss=0.0646                                        \n",
      "Batch # 82 loss=0.0658                                        \n",
      "Batch # 83 loss=0.0669                                        \n",
      "Batch # 84 loss=0.0633                                        \n",
      "Batch # 85 loss=0.0638                                        \n",
      "Batch # 86 loss=0.0688                                        \n",
      "Batch # 87 loss=0.1003                                        \n",
      "Batch # 88 loss=0.0775                                        \n",
      "Batch # 89 loss=0.0920                                        \n",
      "Batch # 90 loss=0.0691                                        \n",
      "Batch # 91 loss=0.0771                                        \n",
      "Batch # 92 loss=0.0789                                        \n",
      "Batch # 93 loss=0.0751                                        \n",
      "Batch # 94 loss=0.0665                                        \n",
      "Batch # 95 loss=0.0659                                        \n",
      "Batch # 96 loss=0.0657                                        \n",
      "Batch # 97 loss=0.0643                                        \n",
      "Batch # 98 loss=0.0642                                        \n",
      "Batch # 99 loss=0.0621                                        \n",
      "Batch # 100 loss=0.0603                                        \n",
      "Batch # 101 loss=0.0607                                        \n",
      "Batch # 102 loss=0.0594                                        \n",
      "Batch # 103 loss=0.0586                                        \n",
      "Batch # 104 loss=0.0573                                        \n",
      "Batch # 105 loss=0.0567                                        \n",
      "Batch # 106 loss=0.0562                                        \n",
      "Batch # 107 loss=0.0556                                        \n",
      "Batch # 108 loss=0.0552                                        \n",
      "========== EPOCH #2 ========== (0.1009/2.0553)        \n",
      "Batch # 1 loss=0.0554                                        \n",
      "Batch # 2 loss=0.0557                                        \n",
      "Batch # 3 loss=0.0532                                        \n",
      "Batch # 4 loss=0.0539                                        \n",
      "Batch # 5 loss=0.0534                                        \n",
      "Batch # 6 loss=0.0517                                        \n",
      "Batch # 7 loss=0.0527                                        \n",
      "Batch # 8 loss=0.0515                                        \n",
      "Batch # 9 loss=0.0506                                        \n",
      "Batch # 10 loss=0.0510                                        \n",
      "Batch # 11 loss=0.0494                                        \n",
      "Batch # 12 loss=0.0492                                        \n",
      "Batch # 13 loss=0.0491                                        \n",
      "Batch # 14 loss=0.0480                                        \n",
      "Batch # 15 loss=0.0482                                        \n",
      "Batch # 16 loss=0.0476                                        \n",
      "Batch # 17 loss=0.0469                                        \n",
      "Batch # 18 loss=0.0470                                        \n",
      "Batch # 19 loss=0.0463                                        \n",
      "Batch # 20 loss=0.0459                                        \n",
      "Batch # 21 loss=0.0458                                        \n",
      "Batch # 22 loss=0.0453                                        \n",
      "Batch # 23 loss=0.0450                                        \n",
      "Batch # 24 loss=0.0448                                        \n",
      "Batch # 25 loss=0.0443                                        \n",
      "Batch # 26 loss=0.0440                                        \n",
      "Batch # 27 loss=0.0437                                        \n",
      "Batch # 28 loss=0.0434                                        \n",
      "Batch # 29 loss=0.0431                                        \n",
      "Batch # 30 loss=0.0429                                        \n",
      "Batch # 31 loss=0.0426                                        \n",
      "Batch # 32 loss=0.0423                                        \n",
      "Batch # 33 loss=0.0421                                        \n",
      "Batch # 34 loss=0.0418                                        \n",
      "Batch # 35 loss=0.0416                                        \n",
      "Batch # 36 loss=0.0414                                        \n",
      "Batch # 37 loss=0.0411                                        \n",
      "Batch # 38 loss=0.0409                                        \n",
      "Batch # 39 loss=0.0406                                        \n",
      "Batch # 40 loss=0.0403                                        \n",
      "Batch # 41 loss=0.0401                                        \n",
      "Batch # 42 loss=0.0398                                        \n",
      "Batch # 43 loss=0.0396                                        \n",
      "Batch # 44 loss=0.0394                                        \n",
      "Batch # 45 loss=0.0391                                        \n",
      "Batch # 46 loss=0.0389                                        \n",
      "Batch # 47 loss=0.0386                                        \n",
      "Batch # 48 loss=0.0385                                        \n",
      "Batch # 49 loss=0.0383                                        \n",
      "Batch # 50 loss=0.0380                                        \n",
      "Batch # 51 loss=0.0379                                        \n",
      "Batch # 52 loss=0.0376                                        \n",
      "Batch # 53 loss=0.0374                                        \n",
      "Batch # 54 loss=0.0372                                        \n",
      "Batch # 55 loss=0.0371                                        \n",
      "Batch # 56 loss=0.0369                                        \n",
      "Batch # 57 loss=0.0367                                        \n",
      "Batch # 58 loss=0.0365                                        \n",
      "Batch # 59 loss=0.0362                                        \n",
      "Batch # 60 loss=0.0359                                        \n",
      "Batch # 61 loss=0.0357                                        \n",
      "Batch # 62 loss=0.0356                                        \n",
      "Batch # 63 loss=0.0354                                        \n",
      "Batch # 64 loss=0.0351                                        \n",
      "Batch # 65 loss=0.0349                                        \n",
      "Batch # 66 loss=0.0346                                        \n",
      "Batch # 67 loss=0.0345                                        \n",
      "Batch # 68 loss=0.0343                                        \n",
      "Batch # 69 loss=0.0341                                        \n",
      "Batch # 70 loss=0.0340                                        \n",
      "Batch # 71 loss=0.0338                                        \n",
      "Batch # 72 loss=0.0336                                        \n",
      "Batch # 73 loss=0.0334                                        \n",
      "Batch # 74 loss=0.0333                                        \n",
      "Batch # 75 loss=0.0331                                        \n",
      "Batch # 76 loss=0.0330                                        \n",
      "Batch # 77 loss=0.0329                                        \n",
      "Batch # 78 loss=0.0327                                        \n",
      "Batch # 79 loss=0.0330                                        \n",
      "Batch # 80 loss=0.0335                                        \n",
      "Batch # 81 loss=0.0357                                        \n",
      "Batch # 82 loss=0.0378                                        \n",
      "Batch # 83 loss=0.0411                                        \n",
      "Batch # 84 loss=0.0373                                        \n",
      "Batch # 85 loss=0.0333                                        \n",
      "Batch # 86 loss=0.0321                                        \n",
      "Batch # 87 loss=0.0358                                        \n",
      "Batch # 88 loss=0.0404                                        \n",
      "Batch # 89 loss=0.0350                                        \n",
      "Batch # 90 loss=0.0317                                        \n",
      "Batch # 91 loss=0.0326                                        \n",
      "Batch # 92 loss=0.0343                                        \n",
      "Batch # 93 loss=0.0339                                        \n",
      "Batch # 94 loss=0.0311                                        \n",
      "Batch # 95 loss=0.0308                                        \n",
      "Batch # 96 loss=0.0318                                        \n",
      "Batch # 97 loss=0.0314                                        \n",
      "Batch # 98 loss=0.0304                                        \n",
      "Batch # 99 loss=0.0299                                        \n",
      "Batch # 100 loss=0.0299                                        \n",
      "Batch # 101 loss=0.0302                                        \n",
      "Batch # 102 loss=0.0302                                        \n",
      "Batch # 103 loss=0.0299                                        \n",
      "Batch # 104 loss=0.0293                                        \n",
      "Batch # 105 loss=0.0290                                        \n",
      "Batch # 106 loss=0.0291                                        \n",
      "Batch # 107 loss=0.0291                                        \n",
      "Batch # 108 loss=0.0292                                        \n",
      "========== EPOCH #3 ========== (0.0390/4.0800)        \n",
      "Batch # 1 loss=0.0316                                        \n",
      "Batch # 2 loss=0.0418                                        \n",
      "Batch # 3 loss=0.0648                                        \n",
      "Batch # 4 loss=0.0653                                        \n",
      "Batch # 5 loss=0.0566                                        \n",
      "Batch # 6 loss=0.0474                                        \n",
      "Batch # 7 loss=0.0985                                        \n",
      "Batch # 8 loss=0.1315                                        \n",
      "Batch # 9 loss=0.0806                                        \n",
      "Batch # 10 loss=0.1913                                        \n",
      "Batch # 11 loss=0.2086                                        \n",
      "Batch # 12 loss=0.2290                                        \n",
      "Batch # 13 loss=0.4123                                        \n",
      "Batch # 14 loss=0.3248                                        \n",
      "Batch # 15 loss=0.2789                                        \n",
      "Batch # 16 loss=0.3412                                        \n",
      "Batch # 17 loss=0.3313                                        \n",
      "Batch # 18 loss=0.3481                                        \n",
      "Batch # 19 loss=0.3714                                        \n",
      "Batch # 20 loss=0.5542                                        \n",
      "Batch # 21 loss=0.4343                                        \n",
      "Batch # 22 loss=0.4503                                        \n",
      "Batch # 23 loss=0.3313                                        \n",
      "Batch # 24 loss=0.4924                                        \n",
      "Batch # 25 loss=0.2877                                        \n",
      "Batch # 26 loss=0.4482                                        \n",
      "Batch # 27 loss=0.6194                                        \n",
      "Batch # 28 loss=1.1819                                        \n",
      "Batch # 29 loss=0.6064                                        \n",
      "Batch # 30 loss=0.3958                                        \n",
      "Batch # 31 loss=0.4787                                        \n",
      "Batch # 32 loss=0.2509                                        \n",
      "Batch # 33 loss=0.3481                                        \n",
      "Batch # 34 loss=0.3068                                        \n",
      "Batch # 35 loss=0.2686                                        \n",
      "Batch # 36 loss=0.3013                                        \n",
      "Batch # 37 loss=0.2061                                        \n",
      "Batch # 38 loss=0.2265                                        \n",
      "Batch # 39 loss=0.2257                                        \n",
      "Batch # 40 loss=0.2294                                        \n",
      "Batch # 41 loss=0.2347                                        \n",
      "Batch # 42 loss=0.1944                                        \n",
      "Batch # 43 loss=0.1858                                        \n",
      "Batch # 44 loss=0.1680                                        \n",
      "Batch # 45 loss=0.1725                                        \n",
      "Batch # 46 loss=0.1725                                        \n",
      "Batch # 47 loss=0.1658                                        \n",
      "Batch # 48 loss=0.1695                                        \n",
      "Batch # 49 loss=0.1660                                        \n",
      "Batch # 50 loss=0.1588                                        \n",
      "Batch # 51 loss=0.1444                                        \n",
      "Batch # 52 loss=0.1732                                        \n",
      "Batch # 53 loss=0.1595                                        \n",
      "Batch # 54 loss=0.1681                                        \n",
      "Batch # 55 loss=0.1680                                        \n",
      "Batch # 56 loss=0.1529                                        \n",
      "Batch # 57 loss=0.1552                                        \n",
      "Batch # 58 loss=0.1506                                        \n",
      "Batch # 59 loss=0.1406                                        \n",
      "Batch # 60 loss=0.1443                                        \n",
      "Batch # 61 loss=0.1439                                        \n",
      "Batch # 62 loss=0.1383                                        \n",
      "Batch # 63 loss=0.1412                                        \n",
      "Batch # 64 loss=0.1395                                        \n",
      "Batch # 65 loss=0.1334                                        \n",
      "Batch # 66 loss=0.1337                                        \n",
      "Batch # 67 loss=0.1320                                        \n",
      "Batch # 68 loss=0.1291                                        \n",
      "Batch # 69 loss=0.1303                                        \n",
      "Batch # 70 loss=0.1300                                        \n",
      "Batch # 71 loss=0.1279                                        \n",
      "Batch # 72 loss=0.1281                                        \n",
      "Batch # 73 loss=0.1272                                        \n",
      "Batch # 74 loss=0.1252                                        \n",
      "Batch # 75 loss=0.1251                                        \n",
      "Batch # 76 loss=0.1247                                        \n",
      "Batch # 77 loss=0.1237                                        \n",
      "Batch # 78 loss=0.1238                                        \n",
      "Batch # 79 loss=0.1235                                        \n",
      "Batch # 80 loss=0.1225                                        \n",
      "Batch # 81 loss=0.1224                                        \n",
      "Batch # 82 loss=0.1222                                        \n",
      "Batch # 83 loss=0.1214                                        \n",
      "Batch # 84 loss=0.1212                                        \n",
      "Batch # 85 loss=0.1208                                        \n",
      "Batch # 86 loss=0.1201                                        \n",
      "Batch # 87 loss=0.1199                                        \n",
      "Batch # 88 loss=0.1194                                        \n",
      "Batch # 89 loss=0.1189                                        \n",
      "Batch # 90 loss=0.1189                                        \n",
      "Batch # 91 loss=0.1185                                        \n",
      "Batch # 92 loss=0.1181                                        \n",
      "Batch # 93 loss=0.1180                                        \n",
      "Batch # 94 loss=0.1175                                        \n",
      "Batch # 95 loss=0.1171                                        \n",
      "Batch # 96 loss=0.1169                                        \n",
      "Batch # 97 loss=0.1166                                        \n",
      "Batch # 98 loss=0.1164                                        \n",
      "Batch # 99 loss=0.1162                                        \n",
      "Batch # 100 loss=0.1159                                        \n",
      "Batch # 101 loss=0.1157                                        \n",
      "Batch # 102 loss=0.1154                                        \n",
      "Batch # 103 loss=0.1151                                        \n",
      "Batch # 104 loss=0.1148                                        \n",
      "Batch # 105 loss=0.1146                                        \n",
      "Batch # 106 loss=0.1144                                        \n",
      "Batch # 107 loss=0.1142                                        \n",
      "Batch # 108 loss=0.1139                                        \n",
      "========== EPOCH #4 ========== (0.1995/1.6254)        \n",
      "Batch # 1 loss=0.1137                                        \n",
      "Batch # 2 loss=0.1135                                        \n",
      "Batch # 3 loss=0.1133                                        \n",
      "Batch # 4 loss=0.1132                                        \n",
      "Batch # 5 loss=0.1130                                        \n",
      "Batch # 6 loss=0.1128                                        \n",
      "Batch # 7 loss=0.1126                                        \n",
      "Batch # 8 loss=0.1125                                        \n",
      "Batch # 9 loss=0.1123                                        \n",
      "Batch # 10 loss=0.1121                                        \n",
      "Batch # 11 loss=0.1120                                        \n",
      "Batch # 12 loss=0.1118                                        \n",
      "Batch # 13 loss=0.1117                                        \n",
      "Batch # 14 loss=0.1116                                        \n",
      "Batch # 15 loss=0.1114                                        \n",
      "Batch # 16 loss=0.1113                                        \n",
      "Batch # 17 loss=0.1111                                        \n",
      "Batch # 18 loss=0.1110                                        \n",
      "Batch # 19 loss=0.1108                                        \n",
      "Batch # 20 loss=0.1107                                        \n",
      "Batch # 21 loss=0.1105                                        \n",
      "Batch # 22 loss=0.1104                                        \n",
      "Batch # 23 loss=0.1102                                        \n",
      "Batch # 24 loss=0.1101                                        \n",
      "Batch # 25 loss=0.1100                                        \n",
      "Batch # 26 loss=0.1098                                        \n",
      "Batch # 27 loss=0.1098                                        \n",
      "Batch # 28 loss=0.1097                                        \n",
      "Batch # 29 loss=0.1095                                        \n",
      "Batch # 30 loss=0.1094                                        \n",
      "Batch # 31 loss=0.1092                                        \n",
      "Batch # 32 loss=0.1091                                        \n",
      "Batch # 33 loss=0.1090                                        \n",
      "Batch # 34 loss=0.1089                                        \n",
      "Batch # 35 loss=0.1088                                        \n",
      "Batch # 36 loss=0.1087                                        \n",
      "Batch # 37 loss=0.1086                                        \n",
      "Batch # 38 loss=0.1085                                        \n",
      "Batch # 39 loss=0.1084                                        \n",
      "Batch # 40 loss=0.1083                                        \n",
      "Batch # 41 loss=0.1081                                        \n",
      "Batch # 42 loss=0.1081                                        \n",
      "Batch # 43 loss=0.1079                                        \n",
      "Batch # 44 loss=0.1078                                        \n",
      "Batch # 45 loss=0.1077                                        \n",
      "Batch # 46 loss=0.1076                                        \n",
      "Batch # 47 loss=0.1075                                        \n",
      "Batch # 48 loss=0.1074                                        \n",
      "Batch # 49 loss=0.1073                                        \n",
      "Batch # 50 loss=0.1072                                        \n",
      "Batch # 51 loss=0.1071                                        \n",
      "Batch # 52 loss=0.1070                                        \n",
      "Batch # 53 loss=0.1069                                        \n",
      "Batch # 54 loss=0.1068                                        \n",
      "Batch # 55 loss=0.1068                                        \n",
      "Batch # 56 loss=0.1066                                        \n",
      "Batch # 57 loss=0.1066                                        \n",
      "Batch # 58 loss=0.1065                                        \n",
      "Batch # 59 loss=0.1064                                        \n",
      "Batch # 60 loss=0.1063                                        \n",
      "Batch # 61 loss=0.1062                                        \n",
      "Batch # 62 loss=0.1061                                        \n",
      "Batch # 63 loss=0.1060                                        \n",
      "Batch # 64 loss=0.1060                                        \n",
      "Batch # 65 loss=0.1059                                        \n",
      "Batch # 66 loss=0.1058                                        \n",
      "Batch # 67 loss=0.1057                                        \n",
      "Batch # 68 loss=0.1056                                        \n",
      "Batch # 69 loss=0.1056                                        \n",
      "Batch # 70 loss=0.1055                                        \n",
      "Batch # 71 loss=0.1054                                        \n",
      "Batch # 72 loss=0.1053                                        \n",
      "Batch # 73 loss=0.1052                                        \n",
      "Batch # 74 loss=0.1052                                        \n",
      "Batch # 75 loss=0.1051                                        \n",
      "Batch # 76 loss=0.1050                                        \n",
      "Batch # 77 loss=0.1049                                        \n",
      "Batch # 78 loss=0.1048                                        \n",
      "Batch # 79 loss=0.1048                                        \n",
      "Batch # 80 loss=0.1047                                        \n",
      "Batch # 81 loss=0.1046                                        \n",
      "Batch # 82 loss=0.1045                                        \n",
      "Batch # 83 loss=0.1045                                        \n",
      "Batch # 84 loss=0.1045                                        \n",
      "Batch # 85 loss=0.1044                                        \n",
      "Batch # 86 loss=0.1043                                        \n",
      "Batch # 87 loss=0.1042                                        \n",
      "Batch # 88 loss=0.1041                                        \n",
      "Batch # 89 loss=0.1041                                        \n",
      "Batch # 90 loss=0.1040                                        \n",
      "Batch # 91 loss=0.1039                                        \n",
      "Batch # 92 loss=0.1039                                        \n",
      "Batch # 93 loss=0.1038                                        \n",
      "Batch # 94 loss=0.1037                                        \n",
      "Batch # 95 loss=0.1037                                        \n",
      "Batch # 96 loss=0.1036                                        \n",
      "Batch # 97 loss=0.1035                                        \n",
      "Batch # 98 loss=0.1035                                        \n",
      "Batch # 99 loss=0.1034                                        \n",
      "Batch # 100 loss=0.1033                                        \n",
      "Batch # 101 loss=0.1032                                        \n",
      "Batch # 102 loss=0.1032                                        \n",
      "Batch # 103 loss=0.1031                                        \n",
      "Batch # 104 loss=0.1031                                        \n",
      "Batch # 105 loss=0.1030                                        \n",
      "Batch # 106 loss=0.1029                                        \n",
      "Batch # 107 loss=0.1029                                        \n",
      "Batch # 108 loss=0.1028                                        \n",
      "========== EPOCH #5 ========== (0.1073/2.1315)        \n",
      "Batch # 1 loss=0.1028                                        \n",
      "Batch # 2 loss=0.1027                                        \n",
      "Batch # 3 loss=0.1026                                        \n",
      "Batch # 4 loss=0.1025                                        \n",
      "Batch # 5 loss=0.1024                                        \n",
      "Batch # 6 loss=0.1024                                        \n",
      "Batch # 7 loss=0.1023                                        \n",
      "Batch # 8 loss=0.1022                                        \n",
      "Batch # 9 loss=0.1022                                        \n",
      "Batch # 10 loss=0.1021                                        \n",
      "Batch # 11 loss=0.1021                                        \n",
      "Batch # 12 loss=0.1020                                        \n",
      "Batch # 13 loss=0.1019                                        \n",
      "Batch # 14 loss=0.1019                                        \n",
      "Batch # 15 loss=0.1018                                        \n",
      "Batch # 16 loss=0.1017                                        \n",
      "Batch # 17 loss=0.1017                                        \n",
      "Batch # 18 loss=0.1016                                        \n",
      "Batch # 19 loss=0.1016                                        \n",
      "Batch # 20 loss=0.1015                                        \n",
      "Batch # 21 loss=0.1015                                        \n",
      "Batch # 22 loss=0.1014                                        \n",
      "Batch # 23 loss=0.1014                                        \n",
      "Batch # 24 loss=0.1013                                        \n",
      "Batch # 25 loss=0.1013                                        \n",
      "Batch # 26 loss=0.1012                                        \n",
      "Batch # 27 loss=0.1011                                        \n",
      "Batch # 28 loss=0.1011                                        \n",
      "Batch # 29 loss=0.1010                                        \n",
      "Batch # 30 loss=0.1010                                        \n",
      "Batch # 31 loss=0.1009                                        \n",
      "Batch # 32 loss=0.1009                                        \n",
      "Batch # 33 loss=0.1008                                        \n",
      "Batch # 34 loss=0.1008                                        \n",
      "Batch # 35 loss=0.1007                                        \n",
      "Batch # 36 loss=0.1006                                        \n",
      "Batch # 37 loss=0.1006                                        \n",
      "Batch # 38 loss=0.1005                                        \n",
      "Batch # 39 loss=0.1005                                        \n",
      "Batch # 40 loss=0.1005                                        \n",
      "Batch # 41 loss=0.1004                                        \n",
      "Batch # 42 loss=0.1003                                        \n",
      "Batch # 43 loss=0.1003                                        \n",
      "Batch # 44 loss=0.1002                                        \n",
      "Batch # 45 loss=0.1001                                        \n",
      "Batch # 46 loss=0.1001                                        \n",
      "Batch # 47 loss=0.1000                                        \n",
      "Batch # 48 loss=0.1000                                        \n",
      "Batch # 49 loss=0.0999                                        \n",
      "Batch # 50 loss=0.1000                                        \n",
      "Batch # 51 loss=0.0999                                        \n",
      "Batch # 52 loss=0.0998                                        \n",
      "Batch # 53 loss=0.0998                                        \n",
      "Batch # 54 loss=0.0997                                        \n",
      "Batch # 55 loss=0.0997                                        \n",
      "Batch # 56 loss=0.0996                                        \n",
      "Batch # 57 loss=0.0996                                        \n",
      "Batch # 58 loss=0.0995                                        \n",
      "Batch # 59 loss=0.0994                                        \n",
      "Batch # 60 loss=0.0994                                        \n",
      "Batch # 61 loss=0.0993                                        \n",
      "Batch # 62 loss=0.0992                                        \n",
      "Batch # 63 loss=0.0992                                        \n",
      "Batch # 64 loss=0.0991                                        \n",
      "Batch # 65 loss=0.0991                                        \n",
      "Batch # 66 loss=0.0990                                        \n",
      "Batch # 67 loss=0.0989                                        \n",
      "Batch # 68 loss=0.0989                                        \n",
      "Batch # 69 loss=0.0989                                        \n",
      "Batch # 70 loss=0.0988                                        \n",
      "Batch # 71 loss=0.0987                                        \n",
      "Batch # 72 loss=0.0987                                        \n",
      "Batch # 73 loss=0.0986                                        \n",
      "Batch # 74 loss=0.0986                                        \n",
      "Batch # 75 loss=0.0985                                        \n",
      "Batch # 76 loss=0.0985                                        \n",
      "Batch # 77 loss=0.0984                                        \n",
      "Batch # 78 loss=0.0984                                        \n",
      "Batch # 79 loss=0.0983                                        \n",
      "Batch # 80 loss=0.0983                                        \n",
      "Batch # 81 loss=0.0982                                        \n",
      "Batch # 82 loss=0.0981                                        \n",
      "Batch # 83 loss=0.0981                                        \n",
      "Batch # 84 loss=0.0981                                        \n",
      "Batch # 85 loss=0.0980                                        \n",
      "Batch # 86 loss=0.0980                                        \n",
      "Batch # 87 loss=0.0979                                        \n",
      "Batch # 88 loss=0.0979                                        \n",
      "Batch # 89 loss=0.0978                                        \n",
      "Batch # 90 loss=0.0978                                        \n",
      "Batch # 91 loss=0.0977                                        \n",
      "Batch # 92 loss=0.0977                                        \n",
      "Batch # 93 loss=0.0976                                        \n",
      "Batch # 94 loss=0.0975                                        \n",
      "Batch # 95 loss=0.0975                                        \n",
      "Batch # 96 loss=0.0974                                        \n",
      "Batch # 97 loss=0.0973                                        \n",
      "Batch # 98 loss=0.0973                                        \n",
      "Batch # 99 loss=0.0972                                        \n",
      "Batch # 100 loss=0.0972                                        \n",
      "Batch # 101 loss=0.0971                                        \n",
      "Batch # 102 loss=0.0970                                        \n",
      "Batch # 103 loss=0.0970                                        \n",
      "Batch # 104 loss=0.0969                                        \n",
      "Batch # 105 loss=0.0969                                        \n",
      "Batch # 106 loss=0.0968                                        \n",
      "Batch # 107 loss=0.0967                                        \n",
      "Batch # 108 loss=0.0968                                        \n",
      "========== EPOCH #6 ========== (0.0997/2.5770)        \n",
      "Batch # 1 loss=0.0969                                        \n",
      "Batch # 2 loss=0.0967                                        \n",
      "Batch # 3 loss=0.0966                                        \n",
      "Batch # 4 loss=0.0966                                        \n",
      "Batch # 5 loss=0.0965                                        \n",
      "Batch # 6 loss=0.0965                                        \n",
      "Batch # 7 loss=0.0964                                        \n",
      "Batch # 8 loss=0.0964                                        \n",
      "Batch # 9 loss=0.0963                                        \n",
      "Batch # 10 loss=0.0962                                        \n",
      "Batch # 11 loss=0.0961                                        \n",
      "Batch # 12 loss=0.0960                                        \n",
      "Batch # 13 loss=0.0960                                        \n",
      "Batch # 14 loss=0.0959                                        \n",
      "Batch # 15 loss=0.0959                                        \n",
      "Batch # 16 loss=0.0959                                        \n",
      "Batch # 17 loss=0.0958                                        \n",
      "Batch # 18 loss=0.0957                                        \n",
      "Batch # 19 loss=0.0956                                        \n",
      "Batch # 20 loss=0.0956                                        \n",
      "Batch # 21 loss=0.0955                                        \n",
      "Batch # 22 loss=0.0955                                        \n",
      "Batch # 23 loss=0.0955                                        \n",
      "Batch # 24 loss=0.0954                                        \n",
      "Batch # 25 loss=0.0953                                        \n",
      "Batch # 26 loss=0.0953                                        \n",
      "Batch # 27 loss=0.0952                                        \n",
      "Batch # 28 loss=0.0952                                        \n",
      "Batch # 29 loss=0.0951                                        \n",
      "Batch # 30 loss=0.0950                                        \n",
      "Batch # 31 loss=0.0950                                        \n",
      "Batch # 32 loss=0.0949                                        \n",
      "Batch # 33 loss=0.0949                                        \n",
      "Batch # 34 loss=0.0948                                        \n",
      "Batch # 35 loss=0.0948                                        \n",
      "Batch # 36 loss=0.0948                                        \n",
      "Batch # 37 loss=0.0947                                        \n",
      "Batch # 38 loss=0.0947                                        \n",
      "Batch # 39 loss=0.0946                                        \n",
      "Batch # 40 loss=0.0946                                        \n",
      "Batch # 41 loss=0.0946                                        \n",
      "Batch # 42 loss=0.0944                                        \n",
      "Batch # 43 loss=0.0944                                        \n",
      "Batch # 44 loss=0.0945                                        \n",
      "Batch # 45 loss=0.0942                                        \n",
      "Batch # 46 loss=0.0942                                        \n",
      "Batch # 47 loss=0.0942                                        \n",
      "Batch # 48 loss=0.0942                                        \n",
      "Batch # 49 loss=0.0941                                        \n",
      "Batch # 50 loss=0.0942                                        \n",
      "Batch # 51 loss=0.0940                                        \n",
      "Batch # 52 loss=0.0939                                        \n",
      "Batch # 53 loss=0.0942                                        \n",
      "Batch # 54 loss=0.0939                                        \n",
      "Batch # 55 loss=0.0938                                        \n",
      "Batch # 56 loss=0.0938                                        \n",
      "Batch # 57 loss=0.0936                                        \n",
      "Batch # 58 loss=0.0937                                        \n",
      "Batch # 59 loss=0.0936                                        \n",
      "Batch # 60 loss=0.0935                                        \n",
      "Batch # 61 loss=0.0934                                        \n",
      "Batch # 62 loss=0.0934                                        \n",
      "Batch # 63 loss=0.0934                                        \n",
      "Batch # 64 loss=0.0933                                        \n",
      "Batch # 65 loss=0.0933                                        \n",
      "Batch # 66 loss=0.0934                                        \n",
      "Batch # 67 loss=0.0933                                        \n",
      "Batch # 68 loss=0.0931                                        \n",
      "Batch # 69 loss=0.0931                                        \n",
      "Batch # 70 loss=0.0934                                        \n",
      "Batch # 71 loss=0.0932                                        \n",
      "Batch # 72 loss=0.0930                                        \n",
      "Batch # 73 loss=0.0931                                        \n",
      "Batch # 74 loss=0.0931                                        \n",
      "Batch # 75 loss=0.0929                                        \n",
      "Batch # 76 loss=0.0931                                        \n",
      "Batch # 77 loss=0.0930                                        \n",
      "Batch # 78 loss=0.0927                                        \n",
      "Batch # 79 loss=0.0929                                        \n",
      "Batch # 80 loss=0.0930                                        \n",
      "Batch # 81 loss=0.0925                                        \n",
      "Batch # 82 loss=0.0927                                        \n",
      "Batch # 83 loss=0.0927                                        \n",
      "Batch # 84 loss=0.0926                                        \n",
      "Batch # 85 loss=0.0927                                        \n",
      "Batch # 86 loss=0.0925                                        \n",
      "Batch # 87 loss=0.0925                                        \n",
      "Batch # 88 loss=0.0924                                        \n",
      "Batch # 89 loss=0.0922                                        \n",
      "Batch # 90 loss=0.0925                                        \n",
      "Batch # 91 loss=0.0921                                        \n",
      "Batch # 92 loss=0.0920                                        \n",
      "Batch # 93 loss=0.0920                                        \n",
      "Batch # 94 loss=0.0919                                        \n",
      "Batch # 95 loss=0.0918                                        \n",
      "Batch # 96 loss=0.0919                                        \n",
      "Batch # 97 loss=0.0918                                        \n",
      "Batch # 98 loss=0.0917                                        \n",
      "Batch # 99 loss=0.0917                                        \n",
      "Batch # 100 loss=0.0916                                        \n",
      "Batch # 101 loss=0.0916                                        \n",
      "Batch # 102 loss=0.0915                                        \n",
      "Batch # 103 loss=0.0915                                        \n",
      "Batch # 104 loss=0.0915                                        \n",
      "Batch # 105 loss=0.0914                                        \n",
      "Batch # 106 loss=0.0913                                        \n",
      "Batch # 107 loss=0.0913                                        \n",
      "Batch # 108 loss=0.0912                                        \n",
      "========== EPOCH #7 ========== (0.0939/3.0393)        \n",
      "Batch # 1 loss=0.0914                                        \n",
      "Batch # 2 loss=0.0919                                        \n",
      "Batch # 3 loss=0.0918                                        \n",
      "Batch # 4 loss=0.0911                                        \n",
      "Batch # 5 loss=0.0914                                        \n",
      "Batch # 6 loss=0.0914                                        \n",
      "Batch # 7 loss=0.0912                                        \n",
      "Batch # 8 loss=0.0910                                        \n",
      "Batch # 9 loss=0.0910                                        \n",
      "Batch # 10 loss=0.0909                                        \n",
      "Batch loss=0.0910                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m path_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeep_torch_11000_as_glam\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_glam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_glam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_save\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 69\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(params, models, dataset, path_save, save_frequency, restart)\u001b[0m\n\u001b[1;32m     66\u001b[0m my_loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(list_batchs(train_dataset, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[0;32m---> 69\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     my_loss_list\u001b[38;5;241m.\u001b[39mappend(batch_loss)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch # \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_loss_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(models, batch, optimizer, criterion)\u001b[0m\n\u001b[1;32m     43\u001b[0m X, sp_A, E_true, i \u001b[38;5;241m=\u001b[39m get_tensor_from_graph(dataset[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     44\u001b[0m Node_emb \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;241m0\u001b[39m](X, sp_A)\n\u001b[0;32m---> 45\u001b[0m Omega \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mNode_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNode_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m E_pred \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;241m1\u001b[39m](Omega)\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(E_pred, E_true)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 100,\n",
    "    \"learning_rate\": 0.05\n",
    "}\n",
    "node_glam = NodeGLAM(9, 5)\n",
    "edge_glam = EdgeGLAM(2*9+2*5, 1)\n",
    "path_save = \"deep_torch_11000_as_glam\"\n",
    "criterion = torch.nn.BCELoss()\n",
    "train_model(params, [node_glam, edge_glam], dataset, path_save, save_frequency=10, restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fda0b131-87ca-4e41-b92d-989b4367ac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([925, 28])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Omega.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c63dc-349b-4863-bbe0-4fdafd90a8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
